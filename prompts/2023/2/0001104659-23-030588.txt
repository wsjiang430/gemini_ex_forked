Prompt: Summarize the business model from the following text. Answer with a continuous text and with fivehundredtwelve tokens at max. Set your focus on sources of revenue , the intended customer base , products , distribution channels  and details of financing. Use only information from the following the text:

Item 1. Business
In this Annual Report on Form 10-K, references to “we,” “us,” “our,” our “company,” “Mobileye,” the “Company,” and similar terms refer to Mobileye Global Inc. and, unless the context requires otherwise, its consolidated subsidiaries, except with respect to our historical business, operations, financial performance, and financial condition prior to our initial public offering, where such terms refer to Mobileye Group, which combines the operations of Cyclops Holdings Corporation, Mobileye B.V., GG Acquisition Ltd., Moovit App Global Ltd., and their respective subsidiaries, along with certain Intel employees mainly in research and development. References to "Moovit” refer to GG Acquisition Ltd., Moovit App Global Ltd. and their consolidated subsidiaries.
We have a 52- or 53-week fiscal year that ends on the last Saturday in December. Fiscal years 2021 and 2020 were 52-week fiscal years; fiscal year 2022 is a 53-week fiscal year. The additional week in fiscal year 2022 is added to the first quarter, which consisted of 14 weeks. Any references to our performance for the years 2022, 2021 and 2020 are references to our fiscal years ended December 31, 2022, December 25, 2021 and December 26, 2020, respectively, and all references to our financial condition as of the end of 2022 and 2021 are references to the end of such fiscal years. Certain amounts, percentages, and other figures presented in this report have been subject to rounding adjustments. Accordingly, figures shown as totals, dollars, or percentage amounts of changes may not represent the arithmetic summation or calculation of the figures that precede them.
Company Overview
Mobileye is a leader in the development and deployment of advanced driver assistance systems (“ADAS”) and autonomous driving technologies and solutions. We pioneered ADAS technology more than 20 years ago and have continuously expanded the scope of our ADAS offerings, while leading the evolution to autonomous driving solutions. 
Our portfolio of solutions is built upon a comprehensive suite of purpose-built software and hardware technologies designed to provide the capabilities needed to make the future of ADAS and autonomous driving a reality. These technologies can be harnessed to deliver mission-critical capabilities at the edge and in the cloud, advancing the safety of road users, and revolutionizing the driving experience and the movement of people and goods globally. 
While today ADAS is central to the advancement of automotive safety, we believe that the future of mobility is autonomous. However, mass adoption of autonomous vehicles is still nascent. Full autonomy - where a human is not actively engaged in driving the vehicle for extended periods of time - requires the autonomous driving solution to be capable of navigating any environment in any condition at any time. Additionally, developing a technology platform whose decision-making process and resulting actions are verifiable is critical to enabling autonomous driving solutions at scale. The ability to drive autonomously not only requires a substantial amount of data, but also a robust technology platform that can withstand the validation and audit process of global regulatory bodies. Finally, the autonomous driving solution needs to be produced at a cost that makes it affordable. We are building our technology platform to address these fundamental and significant challenges in order to enable the full spectrum of solutions, from ADAS to autonomous driving. 
We believe that our industry-leading technology platform, built upon over 20 years of research, development, data collection and validation, and purpose-built software and hardware design, gives us a differentiated ability to not only deliver excellent safety ratings and maintain a leadership position with our ADAS solutions, but also to make the mass deployment of autonomous driving solutions a reality. We also believe that the breadth of our solutions, combined with our global customer base, represents a significant market opportunity for us. Our platform is modular by design, enabling our customers to productize our most advanced solutions today and then leverage those investments to launch even more advanced systems in a modular and incremental manner. Our solutions are also highly customizable, which allows our customers to benefit from our cutting-edge, verified, and validated core ADAS capabilities while also augmenting and differentiating their offerings. 
We have experienced significant growth since our founding. For 2022, 2021 and 2020, our revenue was $1.9 billion, $1.4 billion and $967 million, respectively, representing year-over-year growth of 35% in 2022 compared to 2021. We currently derive substantially all of our revenue from our commercially deployed ADAS solutions. We recorded net losses of $82 million, $75 million and $196 million in 2022, 2021 and 2020, respectively. Our Adjusted Net Income for 2022, 2021 and 2020 was $605 million, $474 million and $289 million, respectively. Adjusted Net Income is a non-GAAP financial measure; see “Item 7. Management’s Discussion and Analysis 

3



of Financial Condition and Results of Operations – Non-GAAP Financial Measures” for a reconciliation of Adjusted Net Income to Net income (loss). The adjustments to reconcile Net Income (Loss) with Adjusted Net Income are related to amortization of intangible assets, stock-based compensation expenses and expenses related to the Mobileye IPO (as defined below). The amortization of intangible assets consisting of developed technology, customer relationships and brands, is primarily a result of Intel’s acquisition of Mobileye in 2017 and, to a lesser extent, the acquisition of Moovit in 2020. 
As noted elsewhere in this Annual Report on Form 10-K, the year ended December 31, 2022 contains an additional week as a result of 2022 being a 53-week fiscal year while 2021 and 2020 are 52-week fiscal years. However, the inclusion of the additional week does not have a material impact on our revenue and cost of revenue as the timing of deliveries to customers is not consistent from week-to-week. Further, most of our expenses (such as payroll) are incurred on a monthly basis and, as such, the accrual for the additional week does not materially impact our results of operations. 
As of December 31, 2022, our solutions had been installed in approximately 800 vehicle models (including local country, year, and other vehicle model variations), and our System-on-Chips (“SoCs”) had been deployed in over 135 million vehicles. We are actively working with more than 50 Original Equipment Manufacturers (“OEMs”) worldwide on the implementation of our ADAS solutions. For the year ended December 31, 2022, we shipped approximately 33.7 million of our EyeQ® SoC and SuperVisionTM systems, of which the substantial majority were EyeQ® SoCs. This represents an increase from approximately 28.1 million systems that we shipped in 2021 and approximately 19.7 million systems that we shipped in 2020. 
We were founded in Israel in 1999. Our co-founder, Professor Amnon Shashua, is our President and Chief Executive Officer. In 2014, we completed an initial public offering as a foreign private issuer and traded under the symbol MBLY on the New York Stock Exchange. Intel Corporation (“Intel”) acquired Mobileye for $15.3 billion in 2017, after which we became a wholly-owned subsidiary of Intel. We completed the Reorganization (as defined below) and Mobileye IPO in October 2022.
Reorganization and Initial Public Offering
In October 2022, Intel completed the internal reorganization and design of our new public entity (the “Reorganization”) for purposes of the initial public offering of Mobileye (the “Mobileye IPO”). The registration statement related to the Mobileye IPO was declared effective on October 25, 2022, and our Class A common stock began trading on The Nasdaq Global Select Market (“Nasdaq”) under the ticker symbol “MBLY” on October 26, 2022. Prior to the completion of the Mobileye IPO, we were a wholly-owned business of Intel. On November 1, 2022, we closed the sale of additional shares pursuant to the exercise of the underwriters’ over-allotment option. Upon the closing of the Mobileye IPO (after giving effect to the exercise of the over-allotment option), Intel continues to directly or indirectly hold all of the Class B common stock of Mobileye, which as of December 31, 2022 represents approximately 99.3% of the voting power of our common stock.
Our Technology Platform is Built to Enable the Full-Stack of Autonomous Solutions
Our technology platform, which includes our software and hardware intellectual property, leverages our decades of experience as a technology leader for sensing and perception solutions for the automotive industry and our focused efforts to build highly scalable and cost-efficient autonomous solutions. Our technologies are foundational to the development and deployment of our ADAS capabilities and consumer AV. Our platform is built on five fundamental pillars:
●highly advanced, road-tested, sensing and perception technologies built upon years of technology leadership in computer vision and powered by our mission critical software and purpose-built EyeQ® family of SoCs; 
●a high-precision mapping system, our Road Experience Management™ (“REM™”), that generates AV maps from crowd-sourced data that is uploaded and analyzed in the cloud from REM™-equipped production ADAS solutions that are deployed on vehicles on the road; 
●a redundant sensor fusion architecture, which we call True Redundancy™, designed to employ two independent perception subsystems - one based solely on cameras, and the other solely on a radar-lidar subsystem, to enable our goal of building a fully autonomous driving-system that can be validated as safer than human-driven vehicles and deployed in a cost-efficient manner; 

4



●the design of next generation imaging-radars, a solution targeted to reduce the need for multiple lidar sensors, combined with a single front-facing lidar sensor in the redundant sensor configuration of the future, to enable our goal of building a cost-effective fully autonomous driving-system; and 
●our Responsibility-Sensitive Safety (“RSS”) framework, which has continuously been optimized since it was first published in 2017, is used by international bodies that are currently developing standards with respect to the safety of AV, and forms the backbone of our human-like, computationally efficient, driving policy and decision-making engine.


These five pillars form the core of our platform, which is highly customizable, and we intend to deploy them with increasing functionality to continue to enhance our market-leading ADAS solutions and lead the evolution to autonomous driving solutions.
Efficiency and Scale are the Foundation of our Rich Portfolio of Solutions
We are focused on offering full-stack solutions across the ADAS and autonomous driving markets. These include or are expected to include:
●a range of ADAS solutions supporting not only “base” features to meet global regulatory requirements and safety ratings, but also higher-function cloud-enhanced feature sets including crowd-sourced maps and eyes-on/hands-off point-to-point assisted driving solutions; 
●off/hands-off autonomous driving solutions with a human driver still in the driver’s seat that may require driver intervention in certain situations for consumer AV with the ability to drive safely without geofenced limitations; and 
●a set of solutions for AMaaS, including a self-driving system, the self-driving vehicles delivered in partnership with OEMs, and a customer-facing application for the movement of people and goods.

We are already in series production for the set of products noted in the first bullet above and believe we have a clear technology roadmap, and customer relationships in place, to reach series production for all other products noted in the bullets above. Each solution in our product portfolio is accomplished by adding a block of our discrete intellectual property that is either in production today or in advanced development stages. We believe that our broad spectrum of value-creating solutions, each of which is scalable, verifiable, and cost-effective, represents a significant competitive advantage.

5



Efficiency
Our purpose-built EyeQ® family of SoCs have a low power consumption profile and tight software/hardware coupling to achieve “lean compute” for efficiency. The principle of efficiency permeates the overall solution design, including our True RedundancyTM approach, with separate subsystems to increase robustness and simplify validation efforts, and RSS, which separates the perception system’s validation from the driving policy system, and allows for a compute-efficient driving policy. Both of these are critical contributors to achieving efficient solutions.
Scale
We achieve both geographic and economic scale by designing our solutions to operate at a cost and performance level that allows our solutions to become ubiquitous. We have designed our solutions to operate with four scale-driven elements:
●our REM™ crowd-sourced AV maps allow the map-building and map-updating process to be automated. Our AV maps are designed to enable vehicles equipped with our new category of cloud-enhanced ADAS that we call “Cloud-Enhanced Driver Assist” and autonomous driving solutions to drive without the limitations of pre-mapped geofenced zones. These AV maps will support our efforts to deploy Mobileye SuperVisionTM and ChauffeurTM across a broad operational design domain and to deploy Mobileye DriveTM in new cities and geographies quickly; 
●our cost-optimized EyeQ® SoC family is highly scalable and built to be at the core of our full spectrum of current and future ADAS and AV solutions, from base ADAS to autonomous driving; 
●our software-defined imaging radars and associated perception technology are designed to function as a second redundant perception layer. By reducing the lidar content per vehicle, we believe we will be able to reduce costs significantly, and facilitate consumer AV and AMaaS solutions at scale; and 
●our driving policy (RSS-based) is designed for global deployment, as it does not rely on local or regional driving cultural norms. The generalization of our driving policy is being proven in multiple testing sites in North America, Europe and Asia.

We Have a History of Innovation and Market Leadership
Our market position has remained strong across a broad set of customer relationships for many years. We are actively working with more than 50 OEMs worldwide on the implementation of our ADAS solutions and we are recognized for our top-rated safety solutions globally. 
Since 2007, when we first launched the EyeQ®1, we have introduced numerous industry-first ADAS products.

Our Family of Purpose-Built EyeQ® SoCs
Our family of purpose-built EyeQ® SoCs is fundamental to our leadership position in ADAS. Our EyeQ® SoCs incorporate a set of proprietary compute-acceleration models, to enhance the accuracy, quality, and functional safety of our perception solutions, while minimizing the power consumption to address the requirements of the automotive market. The EyeQ® family design enables a scalable 

6



Electronic Control Unit (“ECU”) architecture, thereby supporting a variety of ADAS solution architectures. These solutions range from base, windshield mounted ECUs to multi-SoC central compute ECUs supported currently by EyeQ®5 as well as our announced EyeQ®6, which can be deployed in a scalable way to support eyes-on/hands-off SuperVision™ through a variety of eyes-off/hands-off operational design domains (“ODDs”) for autonomous vehicles, both consumer-owned and fleet-deployed. Our EyeQ®5 SoCs and subsequent generations feature EyeQ Kit™ - an end-to-end software development kit (“SDK”) intended to enable the co-hosting of our partners’ and customers’ workloads alongside our cutting-edge AI technologies. Our SDK provides access to all EyeQ® accelerators for programming and is enabled by a broad ecosystem of standard and proprietary software. EyeQ Kit™ is the evolution of our core competencies and differentiated central compute knowhow. EyeQ Kit™ brings together a team of compilers, simulators, profilers, and debuggers, who have been working together for many years, to develop a single software platform optimized for common workloads and industry standards. EyeQ Kit™ is expected to be used by several OEMs and Tier 1s, and hosts third-party content such as vehicle control systems, driver monitoring systems, parking functions, and visualization features, at the choice of our customers. Our end-to-end software model encourages our customers to innovate on top of our platform, augmenting and differentiating their offerings, while benefiting from our cutting-edge, verified, and validated core technologies such as computer vision, true redundancy perception, REMTM mapping and driving policy. Importantly, we believe EyeQ Kit™ accelerates time to market for our customers at a lower cost than alternative in-house solutions, while strengthening our partnerships by encouraging our customers to customize their offerings on top of our platform.

Road Experience Management™
REM™ is a cloud-based system that leverages the broad installed-base of REM™-equipped vehicles to build Mobileye Roadbook™, our crowd-sourced, high-definition maps of roads from around the world. Our REM™ mapping system harvests small packets of Road Segment Data from millions of vehicles that have been launched by our partner OEMs since 2018 that are equipped with our EyeQ®4 Mid and above SoCs, and special processing software that extracts only the relevant information that is necessary to support increasing levels of ADAS and autonomous driving. The Road Segment Data is uploaded to the cloud where our software automatically creates and updates a detailed and accurate model of the road. Our REM™ mapping system seamlessly creates high-precision AV maps in the cloud at centimeter detail, which are then delivered to the edge to provide vehicles with real-time intelligence, including situational awareness, context, and foresight. Mobileye Roadbook™ was designed to provide the driving solution with a pre-aggregated representation of relevant static and slowly changing elements of the environment (road geometry, boundaries, and semantics) and temporary events such as construction zones and road debris, at a high refresh rate. In 2022, we estimate that the data we have accumulated covers over 90% and 90% of the approximately 0.8 million miles of motorway, trunk, and primary road types in each of the United States and Europe, respectively. This data enables us to create robust high definition maps to support solutions across the product spectrum from cloud-enhanced ADAS to Mobileye SuperVision Lite™ and Mobileye SuperVision™ to Mobileye Drive™ and Mobileye Chauffeur™.

7



By augmenting our base ADAS with REM™ and Mobileye Roadbook™, we have pioneered the new ADAS category of cloud-enhanced ADAS, which we call Cloud-Enhanced Driver Assist. Cloud-Enhanced Driver Assist includes an in-path driver assist function capable of:
●Laterally controlling the vehicle to accurately track the driving path even in cases where lane markings are poorly marked, only partially visible, or completely absent (for example, while driving through intersections); and 

8



●Longitudinally responding to traffic directives and road conditions, such as adjustment of the speed according to speed limits, road curvature, or upcoming speed bumps/hazards, and yielding/stopping in response to traffic signs, traffic lights and pedestrian crossings. 

Cloud-Enhanced Driver Assist also provides foresight of road geometry, and the often-complicated association of semantic indications with the different driving paths (e.g., traffic lights and traffic signs) by relying on data from prior human driving activity in those locations and situations. As we continue to rapidly scale our solutions, the benefits of greater data and intelligence not only accrue to our platform, but also to our OEM customers and consumers through greater safety, as well as increased functionality and accuracy across various road conditions.
Our Roadmap to Enable Mass AV Deployment
We believe autonomous driving requires two further major advancements, each of which we are developing, and includes a regulatory framework for deploying AV at scale and a unique sensor fusion architecture, which enhances the effectiveness of the self-driving system. 
RSS: Our Technology Safety Concept for Deploying AV at Scale 
RSS is a formal, explicit, machine interpretable model governing the safety of our autonomous driving solutions’ driving policy. RSS articulates a set of plausible-worst-case assumptions regarding the behavior of other road-users, thereby enabling assertive, human-like driving while rigorously respecting the boundary between safe driving decisions and dangerous, risk-inducing ones. By doing so, it provides a deterministic model for safe driving decisions. As such, RSS further gives regulators and industry participants a framework for standardizing autonomous driving decision-making safety. RSS is also the key enabler of our lean compute driving policy design, as we distinctly separate comfort driving strategies and tactics from safety-related inhibitions and adjustments. RSS has inspired a global standardization effort of AV safety including IEEE 2846, an industry working group that we lead. We first published our RSS model in 2017, setting another example of our industry leadership in addressing one of the key issues to enable regulatory and public acceptance of eyes-off/hands-off autonomous solutions at scale. 
True Redundancy™: Our Unique Sensor Fusion Architecture 
Our unique architecture design, called True Redundancy™, further enhances the robustness and safety of our self-driving system. Rather than fusing all different sensor modalities prior to creating an “environment model” of the world, we are developing two independent perception subsystems. One subsystem is powered solely by cameras and the other is powered by active sensors (radars and lidars). The fusion of the two separate “sensing states” is performed at a high-level with a simple decision mechanism for safety maneuvers and more complex “comfort” maneuvers for human-like driving. We are developing the Mobileye Drive™ self-driving system with a unified True Redundancy™ system including radar and lidar subsystems. In 2021, we announced the expected initial commercial deployment of our AMaaS offering in Munich and Tel Aviv together with Moovit in addition to our multiple testing sites in North America, Europe and Asia. 
A byproduct of our True Redundancy™ architecture is enabling subsystems of our AV development to “scale down” to ADAS, thus creating a seamless and scalable solution portfolio from ADAS to autonomous driving. For example, our Premium Driver Assist offering, Mobileye SuperVision™, launched by Geely Group for its ZEEKR premium electric vehicle brand, is a productization of the camera-based subsystem of our autonomous driving development offering fully operational point-to-point assisted driving navigation. Since the ADAS market is extremely cost-sensitive and cameras are considered the most cost-efficient and versatile sensors powering the evolution of ADAS, the True Redundancy™ architecture enables us to considerably enhance the evolution of ADAS from front-facing camera solutions to a full surround multi-camera solution supporting fully operational eyes-on / hands-off functions. 
The Mobileye SuperVision™ configuration of sensors and compute can also be transformed into an effective “360 guardian,” helping the driver avoid accidents, as referenced in our Vision Zero paper published on arXiv.org in 2018. To take substantial steps towards “Vision Zero” or the goal of reducing driving fatalities and serious injuries from roadway accidents to zero, we leverage surround sensing, our RSS framework and REM™ AV maps. Our AV maps identify areas of potential dangers (such as lane merges, traffic lights and occluded pedestrians) and adjust the driving accordingly, while RSS provides human-like decisions enabled by surround (360) sensing and the fully-integrated REM™ AV map. We believe Mobileye SuperVision™ has the potential to transform ADAS at its core, potentially leading to adoption driven by regulatory requirements and safety ratings of a Mobileye SuperVision™-

9



like solution in its own category, similar to how safety-ratings and regulation have driven the adoption of base ADAS beginning in 2014. We believe that our cost-efficient design of active sensing technology will help support consumer AV production at scale in the future. 
In addition, the autonomous driving-ADAS interplay rooted in our True Redundancy™ architecture is bi-directional: advanced technologies, which are migrated down from the self-driving systems to ADAS, dramatically enhance our ADAS market proposition, and in turn, these advanced autonomous driving technologies are being validated in commercial, mass market ADAS deployments, greatly contributing to the process of verifying and validating the various elements of our self-driving systems. Moreover, our scalable architecture provides our OEM partners with operational efficiencies as our stacked solution architecture minimizes the OEMs’ integration and validation burden as our solutions can be seamlessly deployed across multiple vehicle segments. 
We are designing a “software-defined” imaging radar with a dynamic range and resolution backed by advanced processing algorithms to enable an independent “sensing state.” We have chosen to focus on the evolution of the radar modality, given its cost structure is significantly below lidar-only systems. We believe our custom designed, imaging radars address not only the performance, but also the cost limitations of a radar-multiple lidar solution for mass AV deployment. Our radar is expected to deliver rich point-cloud models like those customary of lidar, with far higher resolution and a significantly more dynamic range than traditional radar. We believe that this will allow us to eliminate the need for multiple high-cost lidars around the vehicle and require only a single front-facing lidar, thereby significantly lowering the overall cost of the required sensors compared to other solutions that use lidar-centric or lidar-only systems.

Our True Redundancy™ architecture with two separate subsystems combines both cameras and software-defined imaging radar around the vehicle, with a single front-facing lidar for three-way redundancy, which will be powered by our next generation EyeQ® chips. This unique True Redundancy™ architecture is designed to bring the cost structure of a full self-driving system to a consumer level by having the imaging radars replace the multiple, expensive lidars around the vehicle and require only a single front-facing lidar, enabling eyes-off/hands-off autonomous solutions with advanced ODDs to be launched at scale. Until completion of development of 

10



our software-defined imaging radar, we expect the implementation of our True Redundancy™ architecture to employ third-party lidars and commercially available radars.

Represents commercially deployed solutions (Driver Assist, Cloud-Enhanced Driver Assist and Mobileye SuperVision) and solutions that we expect to be commercially deployed in the future (Mobileye Chauffeur™, Mobileye Drive™, and AMaaS).
In January 2022, we announced a design win for our consumer AV system, Mobileye Chauffeur™, with ZEEKR, Geely Group’s premium electric vehicle brand. Mobileye Chauffeur™ is expected to be capable of eyes-off/hands-off driving with a human driver still in the driver’s seat, in a gradually expanding ODD, and is expected to use surrounding imaging radars and front-facing lidar. The ODD for such a system can range from a limited ODD (e.g., highway only) to the much more advanced ODDs that we are pursuing through our Mobileye Chauffeur™ solution. By using Mobileye SuperVision™ eyes-on/hands-off “full ODD” system as a basis for Mobileye Chauffeur™, we allow for an incremental and modular transition from one ODD to the next. This can be done by adding more active sensors for redundancy and more compute power to the already validated and road-tested Mobileye SuperVision™. This approach gives our customers a viable, modular, and incremental path toward useful and safe consumer AV solutions.
Building upon Mobileye Chauffeur™, which targets the consumer-owned AV market, we are developing Mobileye Drive™, our eyes-off/hands-off self-driving system with a more advanced ODD targeted for fleet-owned AMaaS and goods delivery networks. While these markets are still nascent, we view the potential use of autonomous driving technology by the operators of passenger and goods transportation networks as unlocking significant efficiencies and safety improvements. While these networks will require multiple layers of technology, we believe the majority of the value will accrue to the companies that provide (1) the self-driving system itself, (2) the mobility intelligence platform and services, and (3) demand and user experience. 
Self-Driving System - Mobileye Drive™ encompasses our core autonomous driving technologies and will deliver all driving functions without the need for any in-vehicle human intervention. We believe our self-driving system has sustainable competitive advantages as a result of the cost efficiency, scalability, and regulatory validation of our technology platform: 
●Cost Efficiency - cost-efficient, low-energy, purpose-built central compute processors; imaging radars targeted to reduce the need for multiple lidar units and require only a single front-facing lidar; 
●Geographic Scalability - REM™-based AV maps that eliminate the need for dedicated high-definition mapping efforts; RSS-based driving policy designed for global deployment by not relying on driving culture or local rules; sensing technologies built on a foundation of a massive data training set from over 40 countries; and 

11



●Regulatory Validation - True Redundancy™, with independent, separate perception subsystems that increases robustness and ease of validation, RSS used by international bodies that are currently developing standards with respect to the safety of AV. 

Mobility Intelligence Platform, Demand and Services - We provide this layer through Moovit, a leading urban mobility app and MaaS solutions provider, which was acquired by Intel in 2020 to support the Mobileye business and which became wholly owned by us as part of the Reorganization. Moovit’s user base and data generation system tracks mobility demand patterns globally, and enables a key mobility intelligence layer that can be used to intelligently predict ride demand and thus help to optimize fleet utilization.
Demand and Rider Experience - Moovit’s global user base also provides a ready consumer base for our business-to-business customers. It also provides the necessary service and user-base layer within our own AMaaS solution. 
While the technology to unlock these markets is approaching commercialization, business models on how services will be delivered are still nascent. Our strategy is to remain supportive of a variety of business models and pursue a variety of commercial programs, with a variety of partners, in a wide range of geographies. We expect our primary go-to-market strategy will be to supply our self-driving systems to producers of AV-ready vehicle platforms for sale to a series of demand-generation customers (with the customers gained through the vehicle producers’ channels or our own). This strategy has gained traction over the last several years, as we have developed customer engagements with entities on the demand side (i.e., public transit operators and transportation network companies such as Sixt, Deutsche Bahn, Beep, Holo / Ruter and others) as well as engagements with customers on the supply side (i.e., producers of AV-ready vehicle platforms such as Schaeffler, Holon and OEM producers of light commercial vehicles). We also continue to pursue the business-to-customer channel with full vertically integrated MaaS activities in partnership with SIXT in Europe and in a Mobileye owned-and-operated network in Israel, although we expect these partially- or fully-owned and operated networks to remain at proof-of-concept volumes as we are committed to maintaining a capital light-model. 
We believe we are well positioned to commercialize these opportunities, and that our scale, cost, and regulatory validation advantages will become evident to the broader market and lead to significant additional opportunities to grow these services globally.

We believe that our industry-leading technology platform, built upon multiple years of research, development, data collection and validation, gives us the unique ability to not only deliver excellent safety ratings with our ADAS solutions, but also to make the mass deployment of autonomous driving solutions a reality. We believe that the breadth of our solutions, combined with our global customer base, represents a significant market opportunity for us.

12



The Autonomous Vehicle Revolution
Autonomous driving is one of the most difficult technological challenges facing the world today. Autonomous driving as a technological concept has been at the forefront of human imagination for decades. Since the early 2000s, a number of automotive and technology companies have invested heavily to try to make this a reality.

Vehicle autonomy can be viewed as a spectrum that uses the same technology building blocks to power the full span of driver assist functions, ranging from those available in hundreds of car models today, through full autonomy powering robotaxis and, eventually, personal autonomous vehicles. The automotive industry breaks down this spectrum into what are known as SAE Levels 1, 2, 3, 4 and 5. We have developed our own, more user-friendly taxonomy. Each level of our taxonomy is further defined and supported by the particular ODD for which it was designed. We refer to basic driver assist features, such as automatic emergency braking or lane keeping assist, together with longitudinal control such as adaptive cruise control as “eyes-on/hands-on”. The eyes-on/hands-on designation indicates the driver remains responsible for all driving functions while the system supports the driver. The next level up is “eyes-on/hands-off” and refers to premium driver assist functions adding additional safety and comfort functionality. This functionality allows the driver to experience hands-free driving while the driver must still monitor the vehicle. The next level of autonomous functionality enables the driver to relinquish control under certain ODDs such as highway driving, which we call “eyes-off/hands-off”. Vehicles equipped with eyes-off/hands-off functionality but that also incorporate a broader set of ODDs can be deployed into the consumer market or the mobility-as-a-service market and operate with no human intervention. We refer to autonomy that does not require human driver intervention in any situation also as “eyes-off/hands-off”. For Consumer-owned vehicles, the expectation is that a human “operator” of the car will always be present. For Mobility-as-a-Service deployed vehicles there will be no human “operator” present which drives the need for teleoperators. We refer to this as “eyes-off/hands-off/no driver”. 
We believe that the path to full autonomy at scale will begin with increased proliferation of the middle category - eyes-on/hands-off premium driver assist - enabling hands-free highway driving, for example, and then will gradually extend to other types of roadways, such as rural, urban, and arterial roads. This will allow continued technological development and public trust and familiarity to grow and pave the way toward full autonomy. Our ADAS solutions, which have been deployed in more than 135 million vehicles, are important building blocks for these more advanced autonomous systems. We believe the key factors in the growth of autonomous driving will be increased safety, consumer demand, and other economic and social benefits, such as increased mobility for older adults and persons with disabilities, less traffic congestion, and the reduction of land use for parking.

13



Models for AV Adoption
We believe that the availability of AVs will cause a significant transformation in mobility, including vehicle ownership and utilization. We expect that AV technology will eventually be accessed by consumers through shared-vehicle AMaaS networks, as well as in consumer-owned and operated AVs. It is our view that, to reach the full potential of autonomous driving over the long-term, the technology solutions that enable these separate markets should converge over time, and that is reflected in our strategy. 
Autonomous driving has the potential to dramatically increase the proliferation of shared mobility, creating greater utilization of what is currently a significantly underutilized asset, the car. We believe that this model will ultimately manifest itself in the form of networks operated by a variety of different automotive and technology companies, where the consumer will be able to hail on-demand transportation at the click of a button, instead of owning a vehicle. 
In addition, we believe consumer-owned and operated AVs will fundamentally change how individuals utilize their vehicles. Automation would allow the individual to be significantly more productive during their commute or other time spent in the car, given that the vehicle could operate eyes-off/hands-off in an increasingly wide ODD. Providing consumers with access to affordable autonomous vehicles can create significant value by decreasing time spent focused on the driving function and increasing safety. 
As autonomous driving technology advances, a number of new transportation use cases are expected to emerge around the type of vehicle ownership, what is transported, and where and when the vehicle can operate. We believe that the most important factors in operating AMaaS networks will be the technology that powers the vehicles, as well as the scale of the network which will influence the availability of vehicles. As fleet operators increase network scale and availability of vehicles, the value of the platform to the user base will rise. We believe that mobility supply is developing in two main segments - automated public transport operators and automated transportation network companies - with very few companies able to operate within both over the long-term. It is our view that a flexible solution that supports both consumer AVs and AMaaS will be necessary to reach the full potential of autonomous driving over the long-term.
Challenges to Making Autonomous Vehicles Ubiquitous 
To make autonomous vehicles at scale a reality, we believe that there are three core challenges that must be addressed: 
●Regulatory Endorsement - Autonomous driving solutions must be architected, by design, to be verifiably safe, in a manner that fosters broad societal and regulatory endorsement. Regulation is an often-overlooked factor. While laws and regulations are specific to human drivers, there are challenges to balance safety and practicality of an AV in a manner that is acceptable to society. We believe it will be easier to develop laws and regulations governing a fleet of robotaxis than privately owned vehicles. A fleet operator would receive a limited license per use case, per geographic region and will be subject to extensive reporting and back-office remote operations. In contrast, licensing AVs to consumers would require a complete overhaul of the complex laws and regulations that currently govern drivers. Autonomy must wait until regulation and technology reach an equilibrium, which we believe will first be achieved through AMaaS deployments. Self-driving regulation is inherently complicated, and driving policy depends on “what would happen next” reasoning, which is not factual. Two humans might provide two different answers when asked whether an AV should yield to a car at an intersection or take the right of way. As a result, there is no clear definition of “error,” but rather, it is open to interpretation or depends on after-the-fact judgment. All motor vehicle drivers owe a duty of care to other road users, and autonomous vehicles will need to be held to the same standard. Statistically, autonomous vehicles should be safer than human drivers. For driving policy, however, being “safer” does not always mean being better. As a society, we balance safety and practicality by determining what the “reasonable risk” we are willing to take is, and this is the type of question regulators will be required to address when licensing AV to navigate our roads. 
●Geographic Scale - Geographic scale refers to the challenge of creating high-definition maps with great detail and accuracy, and keeping those maps continuously updated, which is crucial for series production AVs. AMaaS vehicles can be confined to geofenced areas, which allows AVs to reach prominence through the robotaxi industry before expanding the operational driving domain to outside of those areas. While robotaxi operators may be successful providing their services in limited geofenced areas, broad-based consumer AV adoption requires the ability to drive safely anywhere, and in diverse environments, rather than only in geofenced areas. 
●Cost - The cost of a self-driving system commonly employed by robotaxis, with its cameras, radars, lidars, and high-performance computing is currently in the tens of thousands of dollars. This cost level is acceptable for the monetization model of a driverless ride-hailing service, but is far too expensive for series-production passenger cars. In order for autonomous driving consumer 

14



vehicles to scale in volume, we believe the cost of the self-driving system needs to be reduced significantly, such as to several thousands of dollars, an order of magnitude lower than the cost of market solutions to date. The ability to scale at low-cost, both from the on-board technology perspective and the cost of mapping, is critical to the mass adoption of AVs. AVs need to be safe, yet affordable, to achieve adoption among individuals and not just fleet operators.

Our Solutions 
We are building a robust portfolio of end-to-end ADAS and autonomous driving solutions to provide the capabilities needed for the future of autonomous driving, leveraging a comprehensive suite of purpose-built software and hardware technologies. We pioneered “base” ADAS features to meet global regulatory requirements and safety ratings with our Driver Assist solution and we have since created a new category of ADAS with our Cloud-Enhanced Driver Assist and Premium Driver Assist offerings. We will be adding a new innovative Premium ADAS Solution, SuperVision™ Lite, which will utilize the SuperVision™ software stack with a scaled-down sensor suite and an ECU that will include in the future one EyeQ®6 High SoC. This solution will enable eyes-on/hands-off driving on highway road types (as compared to SuperVision™ which is expected to operate on various road types), next-generation automated parking functions, and EyeQ® Kit support, which will enable customers to deploy internally-developed software components on our EyeQ® SoCs while benefiting from our industry-leading technology platform. Additionally, by leveraging Mobileye SuperVision’s™ full-surround computer vision and True Redundancy™, we are developing Mobileye Chauffeur™, our consumer AV solution with a human driver still in the driver’s seat that may require driver intervention in certain situations, and Mobileye Drive™, our eyes-off/hands-off autonomous driving solution. Together with Moovit’s urban mobility and transit application and its global user base, we are developing our own AMaaS offering for consumers built upon Mobileye Drive™. Our current offerings to Tier 1 and OEM customers do not include cameras, radars, lidar systems, or other sensors (except in particular cases). We intend in the future to offer radar and lidar products that are currently in development stages.
Our End-to-End ADAS and AV Solutions 
Driver Assist 
Base Driver Assist functions are foundational to our spectrum of ADAS and AV solutions and include critical safety features such as real-time detection of road users, geometry, semantics, and markings to provide safety alerts and emergency interventions. Our software algorithms and purpose-built hardware are designed to provide the driver with accurate and reliable driver assist solutions, promoting road safety. 
Cloud-Enhanced Driver Assist 
Cloud-Enhanced Driver Assist provides drivers with high-accuracy interpretations of a scene in real-time utilizing centimeter-level drivable path accuracy, foresight of the path ahead, and other semantic information provided by our crowdsourced REM™ mapping system. This additional input to the environmental model enhances speed and quality of the system’s decision-making. Our Cloud-Enhanced Driver Assist solution is category-defining and, with our REM™ mapping system, offers comprehensive in-path assist functionality through lateral vehicle control to maintain the driving path even when lane markings are partly visible or absent and through longitudinal vehicle control to adjust speed based on traffic signs, road markings, road conditions, and other traffic directions or hazards, independently of the driver. It additionally provides information of the road ahead, including geometry and driving semantics, and the often-complicated association of semantic indications to the different driving paths (e.g., traffic lights and traffic signs lane association) by relying on data from prior human driving activity on those roads.

15



Our Revolutionary Mobileye SuperVision™ Solution
Mobileye SuperVisionTM Lite
Mobileye SuperVision™ Lite is our recently-introduced highway-only navigation and assisted driving solution with autonomous parking capabilities supported by our cloud-based enhancements such as REM™. Mobileye SuperVision™ Lite will utilize the SuperVision™ software stack, including our RSS policy model, and will be powered by a Mobileye ECU with one EyeQ®6 SoC, which will process data from the customer’s third party sensor suite featuring six cameras and five radars. Such cameras are expected to consist of two long-range cameras in the front and rear and four short-range surround vision cameras. Mobileye’s SuperVision™ Lite will offer eyes-on/hands-off assisted driving on highway road types, as well as automated lane changes, evasive maneuvering, and red traffic light braking, and will also include all core Driver Assist safety features. This offering is expected to include EyeQ® Kit support, which will enable customers to deploy their own internally-developed (or third party-sourced) software components on our EyeQ® SoCs while benefiting from our industry-leading technology platform. 
Mobileye SuperVisionTM
Mobileye SuperVision™, our Premium Driver Assist offering, is a point-to-point assisted driving navigation solution and includes cloud-based enhancements such as REM™ and supports OTA updates. Mobileye SuperVision™ includes our RSS policy model and supports 360-degree surround sensing with 11 cameras powered by a turnkey ECU with two EyeQ®5 or, in the future, two EyeQ®6 SoCs. Furthermore, in addition to supervised point-to-point assisted driving, Mobileye SuperVision™ is capable of changing lanes, managing priorities, and turning in intersections as well as engaging in automated parking, preventative steering, and braking, and other Driver Assist features. The 11 cameras (seven long range cameras and four short-range surround vision cameras) provide full surround coverage and consist of 120-degree and 28-degree cameras in the front, four 100-degree corner cameras (two front-facing and two rear-facing), a 60-degree rear camera and four wide-view 192-degree short-range cameras mounted on the side mirrors and front and rear bumpers. The mapping is powered by REM™ to create a 360-degree environmental model, and RSS constrains the driving decisions to be compliant with an underlying formally proven model for safe driving decisions. This offering also includes EyeQ® Kit support, which will enable customers to deploy their own internally-developed software on our EyeQ® SoCs while benefiting from our industry-leading technology platform.
Importantly, our SuperVision™ technology also serves as a bridge or foundational technology for Mobileye and its customers to develop a full spectrum of “eyes-off/hands-off” solutions with expanding ODDs. In other words, an OEM that adopts and validates 

16



SuperVision™ is taking a significant step towards Consumer AV as SuperVision™ serves as a validated baseline which can be leveraged to add eyes-off functionality under an increasing set of operating conditions in a modular way.
The first series production launch of this offering occurred in 2021 as Geely Group launched Mobileye SuperVision™ in its ZEEKR premium electric vehicle brand. Over 90,000 SuperVisionTM systems were delivered to ZEEKR in 2022.
Mobileye Chauffeur™ and Mobileye Drive™ 
Our Mobileye Chauffeur™ first generation solution will be based on three EyeQ®6 High SoCs. It will combine our leading computer vision, camera-based perception subsystem with a radar-lidar subsystem. Mobileye Chauffeur™ will provide 360-degrees of coverage through two independent and redundant sensing subsystems offering True Redundancy™ to reduce the validation burden and, along with REM™ AV maps and RSS, to increase scalability and safety. 
Mobileye Drive™, our eyes-off/hands-off solution, will encompass our core autonomous driving technologies found in Mobileye Chauffeur™ (360-degrees of coverage, REM™, True Redundancy™, and RSS) and will deliver the driving functions without the need for any in-vehicle human intervention by adding teleoperability and by minimizing cases where human input would be required. The overall solution will provide a turnkey self-driving system for movement of people and goods that is applicable to various vehicle configurations (such as passenger vehicles, special purpose pods / vehicles, shuttles, and buses) and will be relevant across the range of potential networks (including AMaaS, last-mile delivery and commercial delivery fleets). 
Mobileye Drive™ may be offered across two increasingly vertically integrated product sets each underpinned by our full set of autonomous driving technology solutions: 
●Self-Driving System & Vehicles. We expect to sell our Mobileye Drive™ eyes-off/hands-off self-driving system through business-to-business channels into a range of transportation network operators and vehicle OEMs which would operate a variety of services (e.g., consumer-facing AMaaS, transportation on demand, and the delivery of goods). Example partners on the vehicle OEM side are Benteler (Holon), Schaeffler and a European producer of light commercial vehicles. Example partners on the transportation network company and public transit operator side are Sixt, Deutsche Bahn, Beep, Holo / Ruter, and others. 
●AMaaS. Additionally, Mobileye Drive™ will be designed to interface with Moovit’s MaaS platform, which adds a service layer and a ready-made user base. Moovit’s user base and data generation system tracks mobility demand patterns globally and enables a key mobility intelligence layer that can be used to intelligently predict ride demand and thus help to optimize fleet utilization. We believe this represents one of the world’s largest repositories of transit and mobility data. Moovit’s global user base will provide a ready consumer base for our business-to-business customers. It also will provide the necessary service and user-base layer within our own AMaaS solution where we plan to deploy Mobileye - Drive™-enabled self-driving vehicles in an AMaaS network in partnership with 

17



transportation network companies. An example is our Munich AMaaS project in collaboration with Sixt. Initial commercial deployments of this full-stack service are expected to take place in Munich and Tel Aviv.


Aftermarket Product Portfolio
We develop and sell aftermarket products meant for vehicles that do not come pre-equipped with ADAS technology. These products use Mobileye’s core computer vision processing and purpose-built EyeQ® chips to provide collision avoidance systems. We provide a complete system that can be retrofit and integrated into most vehicles, including EyeQ®, camera, and relevant electronics. These systems are sold primarily to entities that own a medium-to-large size fleet of vehicles.
Our current products include Mobileye 8 Connect and Mobileye Shield+. Mobileye 8 is designed for light and medium-duty vehicles to provide forward collision avoidance warnings, as well as enhanced ADAS features, connectivity, and actionable data insights. Mobileye Shield+ is a system specifically designed for large vehicles that have significant blind spots, such as city buses. These EyeQ®4 based products also have the capability to harvest REMTM data.
Similar to Mobileye’s portfolio of solutions in the core business, the aftermarket product roadmap is robust. Mobileye 9 is a product that is expected to launch in the late 2024, early 2025 timeframe. This product will contain upgraded hardware, supported by EyeQ®6 Low and a 120-degree 8 megapixel camera. The enhanced hardware and software setup will support incremental ADAS features such as traffic sign recognition, stop sign recognition, animal recognition, and more. Beyond enhanced safety the product will also support seamless integration with Driver Monitoring Systems, Video Telematics, and Fleet Management Platforms. Mobileye also plans a successor product to Shield+ called Mobileye FisheyeTM. This product is designed to comply with EU’s General Safety Regulation with respect to Moving Off Information System (MOIS) and Blind Spot Information System (BSIS). These particular regulations require every new large vehicle (as of July 2024) to alert the driver to pedestrians and cyclists in the vehicle’s front and side blind spots.
Overall, we believe our proprietary set of software and hardware technology solutions, results in significant competitive advantages and a wider range of potential offerings compared to other approaches by industry participants attempting to commercialize network-deployed autonomous vehicles. 

18



Our Data Driven Network Effect 
We have assembled a substantial dataset of real-world driving experience, encompassing hundreds of petabytes of data. This data includes tens of millions of clips collected over decades of driving on urban, highway, and arterial roads in various countries throughout the world, during the test and validation phase prior to launch of our dozens of OEM ADAS programs over the last 15 years. This data, plus proprietary search tools, enables us to develop and continuously improve our advanced computer vision algorithms to fit road scenarios and use cases that our system encounters. We have developed sophisticated 2D and 3D automatic-labeling methodologies that, together with a team of thousands of external specialized annotators, allow for fast development cycles for our computer vision engines based on the dataset we have. In addition, our advanced data labeling infrastructure and data mining tools can unlock significant data-driven insights. 
Additionally, we have created a separate dataset of billions of miles of roads driven from, based on our estimates, over one million REM™-enabled vehicles worldwide. We then apply a series of on-cloud algorithms to build this crowd-sourced data into a high-definition, rapidly updating map that contains a rich variety of information, including road geometry, drivable paths, common speeds, right-of-way, and traffic light-to-lane associations.
These two datasets create powerful network effects as we seek to continually improve our solutions as more vehicles are deployed with our technology.
Our REM™-enabled solutions continuously harvest high-precision data that is analyzed in the cloud, creating a large repository of real-world dataset from the analysis of tens of millions of miles of road data per day, varying by road types and geography.

19



As we continue to rapidly scale our offerings, the benefits of greater data and higher intelligence incorporated into our REM™ mapping system not only accrue to our own platform, but also deliver benefits to our customers and to consumers through greater safety and expanded functionality. As the capabilities of our ADAS and autonomous driving solutions improve, we believe that consumer demand for our offerings will increase and lead to greater platform adoption, further accelerating our data collection worldwide. We believe our combination of data and intelligence gives us a significant competitive advantage and differentiates us as a scaled leader capable of advancing full autonomous solution capabilities based on real world road experience data and continuous validation of the safety solution. For example, we utilize our substantial dataset to build and improve the practical implementation of robotic decision making, which is referred to as “driving policy,” that formalizes a driving safety concept. Our autonomous driving solutions are founded on our core sensing and perception technologies and proprietary algorithms, and the safety validation of these solutions through continuous OTA enhancements. We believe the ability to drive autonomously in any environment in any condition at any time across urban, highway and arterial roads globally should be the goal. Doing so not only requires a significant amount of data, but also successfully solving and validating in a scalable way the challenges of delivering a safe solution at each level of autonomy. With a broad installed-base of REM™ connected vehicles that are collecting data and continually enhancing our solutions, we believe we are well positioned to build on our leadership position. 

20



Our Competitive Strengths 
We believe that our leadership in ADAS and autonomous driving is based primarily on our: (1) first-mover advantage; (2) technology, including differentiated technological cores and solution architectures; (3) comprehensive portfolio of solutions; (4) delivery, including agility, response times, and time-to-market; and (5) inherent cost-driven advantages. These significant advantages form the basis for our competitive strengths described below: 
●Coupling of software and hardware delivers optimized performance and efficiency - We design our own purpose-built SoCs and develop a software stack to optimally match the architecture of the SoCs. This results in an optimized cost/performance paradigm, allowing for a range of products that can be produced at high volume. Our coupled software and hardware architecture is highly differentiated from general purpose SoCs and software stacks that are not optimized for a specific use case. Our approach results in low power consumption and lean compute, yet is able to support a very powerful range of solutions for the ADAS and AV markets. 
●Scalable EyeQ® SoC design addresses the entire spectrum of ADAS and autonomous driving - Our proprietary accelerator cores are optimized for a wide variety of computer vision, signal processing, and machine learning tasks, including deep neural networks. Our EyeQ® architecture is highly scalable, powers our solutions, ranging from our base ADAS to highly advanced autonomous driving solutions, and is designed to support the increasingly computationally intensive demands of ADAS and autonomous driving solutions on the same architecture. 
●Industry leading computer vision capabilities - ADAS solutions are responsible for saving lives and must meet very high-performance metrics with extreme levels of efficiency, and pass increasing oversight from regulatory bodies - “good enough” is simply not acceptable. We are a technology leader for computer vision solutions for ADAS, and we have continuously enhanced our leadership position since we launched with customers in 2007 through our ability to meet the extreme performance, accuracy, and cost metrics of our OEM customers. Our products primarily use monocular camera processing that works accurately alone, or together with radar and lidar for redundancy. We have been responsible for many “industry first” launches using monocular vision processing. These include forward collision warning, automatic emergency braking, pedestrian detection, hands-free driving, and numerous other advanced functions based solely on computer vision. We have pioneered many computer vision features such as deep networks for the discovery of “free space” or the space available to the vehicle to drive in, so that a vehicle can determine a driving path. We have enhanced our computer vision capabilities over time to include multiple cameras such as the trifocal camera configuration (three cameras with different fields of view placed side-by-side facing forward), which has been in series production since 2018, and the 11-camera configuration on our Mobileye SuperVision™ solution, which was launched in late 2021. 
●EyeQ Kit™ for developing and deploying differentiated features on top of EyeQ® SoC - Our platform is modular by design, enabling our customers to productize our most advanced solutions today and then leverage those investments to launch even more advanced systems in a modular and incremental manner. Our solutions are also highly customizable, which allows our customers to benefit from our cutting-edge, verified, and validated core technologies such as computer vision, true redundancy perception, REMTM mapping and driving policy, while enabling our customers to augment and differentiate their offerings. Our SDK provides access to all EyeQ® accelerators for programming and is enabled by a broad ecosystem of standard and proprietary software. EyeQ Kit™ brings together a team of compilers, simulators, profilers, and debuggers who have been working together for many years to develop a single software platform optimized for common workloads and industry standards. We believe EyeQ Kit™ accelerates time to market for our customers at a lower cost than alternative in-house solutions, while strengthening our partnerships by encouraging our customers to customize their offerings on top of our unique technology platform and assets.
●“Scale by design” approach - Our technology platform is built to deliver autonomous driving solutions at scale by leveraging our REMTM mapping technology, which will allow our solutions to be driven without the limitations of geofencing; our True Redundancy™ approach, which allows for cost-efficient validation; our RSS and driving policy, which provides a framework for regulatory certainty and lean compute that is critical for mass-deployment; and, our active sensor architecture based on our imaging radars, which we expect will help support cost-efficient consumer AV production at scale in the future. 
●Autonomous driving-ADAS synergies - The autonomous driving-ADAS interplay, which is borne out of our True Redundancy™ architecture, is bi-directional: advanced technologies transfer from autonomous driving to ADAS and significantly enhance our market proposition, and in turn, these advanced autonomous driving technologies are validated in commercial, mass market ADAS deployments and contribute to the process of verifying and validating the various elements of our autonomous driving solution stack. Moreover, our scalable architecture provides our OEM partners with operational efficiencies as modular technology platform 

21



architecture minimizes the OEMs’ integration and validation burden as our solutions can be seamlessly deployed across multiple vehicle segments. 
●Road Experience Management™ creates a powerful network effect and long-term competitive advantage - Our REM™ system is a crucial ingredient that we believe allows for: (1) defining a new category of cloud-enhanced ADAS that we call Cloud-Enhanced Driver Assist, where information in Mobileye Roadbook™ enhances existing ADAS functions such as lane keeping assist and lane-centering and allows for new functions such as the analysis of behavior patterns in intersections and near traffic signs and lights; (2) evolving ADAS to an eyes-on/hands-off point-to-point assisted driving navigation; and (3) the scale deployment of AV. REM™ is complex, requiring advanced processing at the edge (for creating processed data to be sent to the cloud and for localizing the vehicle at centimeter-level accuracy in Mobileye Roadbook™), and computationally intensive processing in the cloud to build Mobileye Roadbook™ from billions of data packets sent from millions of vehicles - all automatically. REM™ benefits from a powerful network effect, where more vehicles with REM™ enabled technology from which we are able to collect and process data, not only improves our own solutions, but also delivers benefits to our customers and to consumers through greater safety and expanded functionality. We believe this network effect creates a powerful competitive advantage, particularly given our leadership position in ADAS, as we are able to efficiently collect large amounts of data from our consumer solutions already deployed on roads globally through their regular use. Our AV maps are a critical component that supports our SuperVisionTM product’s ability to operate across a wide ODD and, therefore, the modular process of expanding this technology to eyes-off/hands-off ChauffeurTM products for a defined ODD. Further, our AV maps support our ability to deploy our AMaaS technology in new cities and geographies quickly. 
●Data and technology advantage - Developing effective ADAS technology is technologically complex, and requires the development of large validation datasets in order to train the required software algorithms effectively, a long-term commitment to validation and qualification with an OEM before series production can even begin, and significant financial resources. We have assembled a substantial dataset of real-world driving experience, encompassing hundreds of petabytes of data, which includes tens of millions of clips collected over decades of driving on urban, highway, and arterial roads all over the world that enable us to develop advanced computer vision algorithms to fit road scenarios and use cases that our system encounters. We have developed sophisticated 2D and 3D automatic-labeling methodologies that, together with a team of thousands of external specialized annotators, allow for fast development cycles for our computer vision engines based on the dataset we have. In addition, our advanced data labeling infrastructure and data mining tools can unlock significant data-driven insights. In parallel, we have created a rich dataset of roads driven from REM™-enabled vehicles that we estimate covers over 90% and 90% of the approximately 0.8 million miles of motorway, trunk, and primary road types in each of the United States and Europe, respectively. This data enables us to create robust high definition maps to support solutions across the product spectrum from cloud-enhanced ADAS to Mobileye SuperVision Lite™ and Mobileye SuperVision™ to Mobileye Drive™ and Mobileye Chauffeur™. Our dataset creates a powerful network effect as we seek to continually improve our solutions as more vehicles are deployed with our technology. 
●RSS and driving policy are designed for global deployment - We published our RSS model in 2017, to address the regulatory and public debate regarding, and enable the acceptance of, eyes-off/hands-off autonomous solutions. RSS is the key enabler of our lean compute driving policy design, where we distinctly separate driving comfort features from safety-related inhibitions and adjustments. Our framework monitors and establishes driving policy by identifying intentions in order to only predict the plausible actions of road users, significantly reducing possible options and computational demands. Our RSS-based driving policy is designed for global deployment, as it does not need to be tailored to specific driving cultures. In 2021, we announced the expected initial commercial deployment of our AMaaS offering in Munich and Tel Aviv together with Moovit in addition to our multiple testing sites in North America, Europe and Asia. 
●Purpose-built imaging-radar unlocks consumer AV at scale - We are developing software-defined imaging-radar with cutting-edge dynamic range and resolution. Our differentiated True Redundancy™ architecture, which is adaptable to different lidar architectures, will leverage our imaging-radar, which we believe will give us the ability to significantly reduce the cost of the overall sensor suite by replacing multiple, expensive lidars around the vehicle, with only a single front-facing lidar sensor, which we believe will support consumer AV production at scale. 
●Moovit provides a stand-ready user base for our AMaaS solutions - Moovit is our urban mobility and transit application. Moovit’s user base and data generation system tracks mobility demand patterns globally and enables a key mobility intelligence layer that can be used to intelligently predict ride demand and thus help to optimize fleet utilization. We believe this represents one of the world’s largest repositories of transit and mobility data. Moovit also offers a MaaS solution to cities, and transit agencies covering 

22



planning, operations, and optimization of their mobility systems. Moovit’s applications provide powerful AI-powered urban mobility services covering planning, operations, and analytics for multimodal trips. 
●Deep, collaborative ecosystem relationships - Our deep global relationships with key partners across the value chain, from component suppliers, through Tier 1 customers and up to OEMs, offer us a broad and diverse set of collaboration opportunities for high-performance computing, networking, and advanced packaging technologies, among others, from the vehicle to the cloud. Together with our partners, we believe that we can accelerate the pace of autonomous innovation and market adoption

Our Growth Strategies 
Key levers of our growth strategy are: 
●Benefit from regulatory and safety rating changes promoting base ADAS - We intend to continue to lead and deliver upon global regulatory and safety requirements for base ADAS features by maintaining and enhancing our vision only solution. We expect a strong increase in base ADAS fitment rates due to global regulatory and safety requirements, as OEMs move to adopt standard ADAS technology for the vast majority of new model launches. We plan to continue to leverage our technology leadership and strong customer relationships to position us for additional design wins with high production volumes. We believe that our comprehensive stack of solutions and proven success at scale will enable us to further solidify our industry leadership. 
●Capitalize on Cloud-Enhanced Driver Assist features - We have pioneered a cloud-enhanced ADAS solution, which offers customers using advanced EyeQ® versions (EyeQ®4 and above) a significant value through our REM™ technology. Our Cloud-Enhanced Driver Assist solution is capable of utilizing our EyeQ® SoCs and entry level camera technologies to deliver feature enhancements over time. Our Cloud-Enhanced Premium ADAS features range in complexity from all road-type lane keeping assist and lane centering, to Cross-Junction Assist, to Traffic Jam Assist. We will continue to grow the depth and breadth of our AV maps in order to deliver leading ADAS capabilities. In the future, we plan to create revenue streams from our OTA capabilities and AV maps through solution upgrades. 
●Further enhance and drive adoption of our Premium Driver Assist solutions - Our Mobileye SuperVision™ solution represents a comprehensive eyes-on/hands-off ADAS solution. It was launched by Geely Group for its ZEEKR premium electric vehicle brand, and we are expanding our collaboration as three additional brands under the Geely Group umbrella are expected to launch Mobileye SuperVision™ technology globally in upcoming electric vehicle models, beginning in 2023. We believe that the high value-add, our continuous efforts to add capabilities, as well as the competitive price point of Mobileye SuperVision™ will allow it to gain strong market traction in the coming years. Our validated SuperVision™ technology can serve as the foundation to enable eyes-off/hands-off capabilities in a modular way. In addition, our Mobileye SuperVision™ configuration of sensors and compute can also be transformed into an effective “360 guardian,” helping the driver avoid accidents, as referenced in our Vision Zero publications. We believe that Mobileye SuperVision™ has the potential to transform ADAS at its core, potentially leading to adoption driven by regulatory requirements and safety ratings of a Mobileye SuperVision™-like solution in its own category, similar to how safety-ratings and regulation have driven the adoption of base ADAS beginning in 2014. 

Additionally, we recently added a new innovative Premium ADAS solution, SuperVision™ Lite, which will utilize the SuperVision™ software stack with a down-scaled sensor suite and an ECU that will include one EyeQ®6 High SoC in the future. The solution will enable eyes-on/hands-off driving on highway road types (as compared to SuperVision™ which is expected to operate on various road types), and next-generation automated parking functions. Mobileye SuperVision™ Lite will provide OEMs with higher levels of autonomy than Cloud-Enhanced Driver Assist, which we believe will expand the application and adoption of our products. 
Our Premium Driver Assist offerings are expected to be available with EyeQ® Kit support, which will enable OEM customers to deploy their own internally-developed software on our EyeQ® SoCs while benefiting from our industry-leading technology platform. 
●Innovate and commercialize our next-generation autonomous driving solutions - Propelled by our next generation EyeQ® SoC, our surround computer vision Mobileye SuperVision™ solution, productization of software-defined imaging radars and our True Redundancy™ architecture, we believe that we will be positioned to deliver an autonomous driving solution that can enable the mass adoption of AV. We plan to continue to develop innovative and cost-optimized solutions to deliver comprehensive capabilities for mass market adoption to our customers. We believe the introduction of our premium ADAS capabilities with our launched Mobileye SuperVision™ solution, which can be scaled to a variety of Mobileye ChauffeurTM Consumer AV solutions, and our eyes-off/hands-off 

23



capabilities with Mobileye Drive™ will help us continue to provide our customers with innovative solutions and enable further growth for us. We plan to continue to build and enhance our full-stack technology platform in order to offer an affordable, time-saving and much safer driving experience, which we believe will propel the mass-market adoption of autonomous driving solutions. 
●Utilize our flexible platform to expand our collaboration with our OEM customers - We have designed our EyeQ® SoCs together with an EyeQ Kit™ to enable co-hosting of third-party software and customer workloads on vehicles equipped with our solutions. We plan to continue to develop our platform to offer our customers the ability to seamlessly address the additional capabilities and features that they demand by customizing their offerings on top of our platform. We are partnering with leading technology suppliers to expand our products by offering features and services alongside our core technology platform such as vehicle control systems, driver monitoring systems, parking functions, and visualization features. In addition, our SDKs enable OEMs to innovate on top of our platform, augmenting and differentiating their offerings, while benefiting from our cutting-edge, verified and validated core technologies such as computer vision, true redundancy perception, REMTM mapping and driving policy. 
●Capitalize on our active sensor technology - We intend to continue to develop and commercialize next-generation active sensors such as software-defined imaging radars, which leverage our AI capabilities. Our software-defined imaging radars are designed to form a standalone “sensing state” layer which can be utilized as a sensing layer on its own, enabling 360-degree coverage, replacing multiple lidar sensors and requiring only a single front-facing lidar. Together with Intel, we also are currently in the early stages of development of frequency-modulated continuous wave (“FMCW”) lidar, which has the potential to replace alternative third-party lidar to further enhance the performance of our sensor suite. We believe enhancing our sensing and perception technology leadership will further strengthen our competitive position and allow us to offer additional differentiated and cost-effective solutions to our customers. 
●Accelerate our roadmap of next generation proprietary EyeQ® SoCs - We believe that we have created the standard for processors focused on computer vision. Our EyeQ® SoCs are purpose-built for sensing and perception technologies and optimized for high throughput and power efficiency. We intend to continue to accelerate our technology leadership with a focus on silicon, packaging, and systems level needs to deliver cost-efficient processing at the edge. EyeQ®6 High be built to address the needs of eyes-on/hands-off and eyes-off/hands-off solutions in a scalable way. Our architecture is highly scalable and is designed to support the increasing and computationally intensive demands of future autonomous driving applications. 
●Utilize our substantial and growing dataset to continuously improve the intelligence and robustness of our solutions - We will continue to grow the depth and breadth of our substantial dataset. We believe that our ability to use this data to create, maintain, and improve our high-precision AV maps through our REM™ mapping system will enable us to further improve our ADAS offerings and position us well for autonomous driving. 
●Establish our Eyes-Off/Hands-Off autonomous and AMaaS solutions - We believe that Mobileye Chauffeur™ and Mobileye Drive™ will unlock new use cases and end-consumers for our OEM and fleet-owner customers, which will be applicable for both the AMaaS and consumer AV markets. We expect to add additional cities to our AMaaS offerings to showcase our industry-leading technology and to help accelerate the pace of AV adoption. We also expect to continue to invest in our ecosystem partnerships with OEMs and transportation network companies in order to foster close collaboration and further commercialize our autonomous technologies. 
●Benefit from opportunities in large emerging markets - We intend to continue to invest in customer relationships in China and India, among other emerging markets, to accelerate ADAS and autonomous driving adoption. In India, Mahindra & Mahindra, one of the country’s largest automakers, has launched the first vehicle made locally to offer ADAS capabilities, which is powered by our EyeQ® SoC. Its accessible price point compared to imported alternatives expands the ADAS reach to a broader range of consumers in one of the most populous countries in the world. We believe our long-term partnerships with large Chinese OEMs such as Geely, Great Wall Motors, and SAIC, and Indian OEMs such as Mahindra & Mahindra position our solutions at the forefront of continued innovation and market growth.

Our Customers 
Our customers include leading OEMs, which we sell to through Tier 1 automotive suppliers that implement our product into automotive vehicles, as well as fleet owners and operators. 

24



OEMs 
Our market position has remained strong across a broad set of customer relationships for many years. We are actively working with more than 50 OEMs worldwide on the implementation of our ADAS solutions.
We work with Tier 1 automotive suppliers to supply our solutions to the following OEMs:

Tier 1 Automotive Suppliers 
We supply certain OEMs with the EyeQ® platform through our arrangements with automotive system integrators, known as Tier 1 automotive suppliers, which are direct suppliers to OEMs. Our Tier 1 customers include Aptiv, Magna, Valeo, Wabco, ZF, and others. 
Mobility-as-a-Service 
We expect to sell the Mobileye Drive™ self-driving vehicles to a range of transportation network companies, public transit operators and vehicle OEMs which intend to operate a variety of services (e.g., consumer-facing AMaaS, transportation on demand, delivery). These partners could produce vehicles themselves and integrate Mobileye Drive™ with our assistance.
Our EyeQ® System-on-Chip Architecture 
EyeQ® 
Each new generation of the EyeQ® SoC is many times faster than its predecessor and tightly integrated with software to offer maximum efficiency. They consist of central processing unit cores and dedicated custom-designed vector accelerators. Our proprietary computational cores are optimized for a wide variety of computer vision, signal processing, and machine learning tasks, including deep neural networks. Our EyeQ® architecture is highly scalable and is designed to support the increasing and computationally intensive demands of ADAS and AV solutions on the same architecture, which provides significant re-use and network effects for our technology platform. 
For the EyeQ® SoC, we have developed four heterogeneous accelerator families for different types of workloads allowing us to optimize performance for each workload by using the most suitable core. 

25



The deployment mix of these accelerators varies by product line based on the functions each EyeQ® SoC supports. Our accelerator architecture allows us to achieve high compute performance with power efficiency. 
Our EyeQ® family of products includes:

●EyeQ®1 - launched in 2007, supported two bundle types: (1) Lane Departure Warning (“LDW”), Traffic Sign Recognition (“TSR”) and Intelligent High-beam Control (“IHC”); and (2) LDW and Vehicle Automatic Emergency Braking (“AEB”) fusion with radar. EyeQ®1 was an industry first supporting camera/radar ACC. 
●EyeQ®2 - launched in 2010, supported a variety of functional bundles, including LDW, TSR, IHC, Forward Collision Warning (“FCW”) and AEB for vehicles and pedestrians (partial braking). EyeQ®2 was an industry first with Pedestrian AEB, and Adaptive Cruise Control (“ACC”). 
●EyeQ®3 - launched in the fourth quarter of 2014. In addition to significant upgrades of all of the above functions, EyeQ®3 supports full braking AEB, structure from motion functionalities, road profile reconstruction, debris detection, general object detection, traffic light detection and REM™. EyeQ®3 was an industry first with Highway Autopilot, Camera-only AEB and full in path assisted driving. 
●EyeQ®4 Mid and EyeQ®4 High - launched in early 2018. EyeQ®4 supports processing from multiple cameras (including multi-focal or ultra-high-resolution front facing and side/rear), as well as other sensor perception modalities through two models: EyeQ®4 Mid and EyeQ®4 High. EyeQ®4 Mid is a one-box windshield solution that offers around 1.1 tera operations per second (“TOPS”) supporting eyes-on/hands-on functionality and EyeQ®4 High offers 2 TOPS supporting REM™ mapping and localization to provide eyes-on/hands-off functionalities. EyeQ®4 was the first SoC to support REM™ Map Harvesting and an industry first supporting 100-degree cameras. 
●EyeQ®5 Mid and EyeQ®5 High - designed to act as the central computing processor to enable fully autonomous driving vehicles, EyeQ®5 comes in two forms: EyeQ®5 Mid and EyeQ®5 High. EyeQ®5 Mid is a one-box windshield solution designed to support up to eyes-on/hands-off functionality. EyeQ®5 High supports Premium ADAS and up to eyes-off/hands-off functionality powering both our Mobileye SuperVision™ and Mobileye Drive™ solutions. Volume production began in 2021. EyeQ®5 is designed on the 7nm fin field-effect transistor (“FinFET”) technology node and offers around 15 TOPS on the EyeQ®5 High and more than 4 on the EyeQ®5 Mid. We have been able to achieve power, performance, and cost targets by employing proprietary computational cores that are optimized for a wide variety of computer vision, signal processing, and machine learning tasks, including deep neural networks. Starting with EyeQ®5, we are supporting a complete SDK to allow customers to differentiate their solutions by deploying their algorithms on EyeQ®5. EyeQ®5 serves as the computational foundation for our scalable camera-only surround sensing system. The system consists of multiple independent computer vision engines and deep networks for algorithmic redundancy. The result is a robust 

26



and comprehensive model of the environment that allows end-to-end autonomous driving. It is also the industry’s first solution supporting 120-degree 8-megapixel cameras. 
●EyeQ®6 Lite and EyeQ®6 High - announced in January 2022, the EyeQ®6 Lite, a one-box windshield optimized solution, is designed to deliver entry and premium eyes-on/hands-on ADAS functionality at ultra-low power and high efficiency. Also announced in January 2022, the EyeQ®6 High will support premium eyes-on/hands-off ADAS capabilities, and is scalable to eyes-off/hands-off capabilities, with full surround and support for visualization and heavy AI workloads with 34 TOPS in 40 watts representing lean compute. This centralized solution is scalable to provide ADAS eyes-on/hands-off through AV eyes-off/hands-off functionalities, multi-camera processing (including short-range surround vision cameras), and will host third-party apps such as parking visualization and driver monitoring. Both the EyeQ®6 Lite and EyeQ®6 High are designed on the 7nm FinFET process technology node. We expect to release the EyeQ®6 Lite/High in 2023 or 2024 and begin production by the end of 2024. 

Each EyeQ® product, whether delivering eyes-on/hands-on or eyes-off/hands-off functionality, is supported by the particular ODD for which the applicable functionality was designed.
Our Partnerships with STMicroelectronics and Intel 
Our long-standing relationship with STMicroelectronics N.V. (“STMicroelectronics”) continues to strengthen with the complexity of our solutions. Our partnership includes close collaboration in product development, design, and manufacturing. For example, we have co-developed the six EyeQ® generations, including the launched EyeQ®6. We also benefit from STMicroelectronics’ advanced packaging and testing capabilities and automotive expertise. Together with STMicroelectronics, we are working on developing and productizing next-generation automotive-grade technology for high volume automotive applications, which we believe will accelerate the pace of autonomous innovation and market adoption. 
Our close partnership with Intel exists on multiple fronts. As a result of our relationship with Intel, we have access to unique and differentiating technologies such as proprietary silicon photonics fabrication technologies, which we may leverage for the early development of our FMCW lidar, which has the potential to replace alternative third-party lidar sensors to further enhance the performance of our sensor suite. We may also license certain technologies from Intel that support design and development of our software-defined radar, including Intel’s mmWave technologies. Additionally, we intend to explore a collaboration with Intel on a technology platform to integrate our EyeQ® SoC with Intel’s market-leading central compute capability, with plans to utilize Intel Foundry Services’ advanced packaging capabilities. This potential platform is intended to enable functions essential to safety, entertainment, and cloud connectivity. Intel’s strength in government affairs and policy development around the world will continue to be of significant value to us as we collaborate with regulators who are preparing frameworks to enable commercial deployment of AVs.
Manufacturing 
Our products are designed and manufactured specifically for automotive applications after extensive validation tests under stringent automotive environmental conditions. 
We partner with STMicroelectronics, a leading supplier and innovator of semiconductor devices for automotive applications, in manufacturing, design and research and development. We have co-developed six generations of our automotive grade SoC, EyeQ®, with STMicroelectronics including EyeQ®5 and EyeQ®6. We design the front-end and STMicroelectronics designs the back-end package and also includes testing, quality assurance, customer care, failure analysis and manufacturing standards. All of our EyeQ® integrated circuits are manufactured by or outsourced to a partner foundry by STMicroelectronics. 
We have also established a relationship with Quanta Computer to develop and assemble our ECUs including our reference design for our Mobileye SuperVision™ solution, which includes our EyeQ®5 SoCs from STMicroelectronics. 
As a result of our relationship with Intel, we have access to unique and differentiating technologies such as proprietary silicon photonics fabrication technologies, capable of putting active and passive optical elements on a chip together, including lasers and optical amplifiers, loaded onto a photonic integrated circuit. We may leverage this technology, which has the ability to put an active laser in a package, for the early development of our FMCW lidar, which has the potential to replace alternative third-party lidar sensors to further enhance the performance of our sensor suite.

27



Regulation and Ratings
Automobile safety is driven by both regulations and the availability to consumers of independent assessments of the safety performance of different car models. These assessments have encouraged OEMs to produce cars that are safer than those required by law. In many countries, these NCAPs have created a “market for safety” as car manufacturers seek to demonstrate that their models satisfy the various NCAPs’ highest ratings. 
National NCAPs will continue to add specific ADAS applications to their evaluation items over the next several years, led by the Euro NCAP. In the EU, pre-market approval is required for all vehicles sold, and many manufacturers choose to satisfy a set of technical criteria determined by the Euro NCAP. The Australian, Japanese, and Korean NCAPs’ have fully harmonized their policies with the Euro NCAP. In the United States, ADAS regulation continues to make large strides. For example, the INVEST in America Act, which was passed in late 2021, requires the U.S. Department of Transportation to issue requirements and standards regarding vehicle safety technologies. On the AV front, our RSS driving policy provides a cornerstone for global standardization efforts of the safety of assisted and automated driving, in particular IEEE 2846, a working group of approximately 30 organizations in the industry that we lead.
At the federal level in the United States, the safety of motor vehicles is regulated by the U.S. Department of Transportation through two federal Agencies - the National Highway Traffic Safety Administration (the “NHTSA”), which regulates all motor vehicles, and the Federal Motor Carrier Safety Administration (the “FMCSA”), which regulates commercial motor vehicles. NHTSA establishes the Federal Motor Vehicle Safety Standards (the “FMVSS”) for motor vehicles and motor vehicle equipment and oversees the actions that manufacturers of motor vehicles and motor vehicle equipment are required to take regarding the reporting of information related to defects or injuries related to their products and the recall and repair of vehicles and equipment that contain safety defects or fail to comply with the FMVSS. FMCSA regulates the safety of commercial motor carriers operating in interstate commerce, the qualifications and safety of commercial motor vehicle drivers, and the safe operation of commercial trucks. 
While there are currently no mandatory federal U.S. regulations expressly pertaining to the safety of autonomous driving systems, the U.S. Department of Transportation has established recommended voluntary guidelines, and the NHTSA or the FMCSA, as applicable, have authority to take enforcement action should an automated driving system pose an unreasonable risk to safety or inhibit the safe operation of a motor vehicle. Certain U.S. states have legal restrictions on autonomous driving vehicles, and many other states are considering them. These variations increase the legal complexity of deploying our solutions. If discrepancies emerge in the legal restrictions adopted by different U.S. states, our plan is to develop our technology to comply with the strictest standards. We will continue to actively monitor regulatory developments in the U.S. and intend to adjust our products and solutions as needed. 
In Europe, certain vehicle safety regulations apply to self-driving braking and steering systems, and certain treaties also restrict the legality of certain higher levels of autonomous driving vehicles. In jurisdictions that follow the regulations of the United Nations Economic Commission for Europe, some regulations restrict the design of advanced driver-assistance or self-driving features, which can compromise or prevent their use entirely. Other applicable laws, both current and proposed, may hinder the path and timeline to introducing self-driving vehicles for sale and use in the markets where they apply. Other markets, including China, continue to consider self-driving regulation. Any implemented regulations may differ materially from those in the United States and Europe, which may further increase the legal complexity of self-driving vehicles and limit or prevent certain features. Autonomous driving laws and regulations are expected to continue to evolve in numerous jurisdictions in the United States and foreign countries and may create restrictions on autonomous driving features that we develop. 
In order for us to operate in international markets outside the United States, we must comply with relevant legal regulations regarding autonomous vehicles as well as technology export control, data security, cybersecurity and other related regulations that apply to global technology companies. We have developed robust compliance processes and procedures related to these regulatory requirements and believe that we are in compliance with such requirements. 
​
​

28



On October 7, 2022, the U.S. Department of Commerce, Bureau of Industry and Security (“BIS”) announced new restrictions on the export of advanced computing integrated circuits and related items to China and certain other jurisdictions. While these restrictions are new and have not yet been interpreted and applied, based on our existing customer base and the export classifications for our existing chip products, we do not believe that these new U.S. export controls will have a material impact on our sales of these products to our existing customers in China. Export control regulations adopted by the United States and other jurisdictions are subject to change and interpretation, and it is possible that future regulatory actions by BIS impacting U.S. exports of integrated circuits and related items to China could have a material impact on our business operations in China. 
Data Privacy
Privacy is fundamental to Mobileye. We collect, process, transmit, and store personal information in connection with the operation of our business and are subject to a variety of local, state, national and international laws, directives and regulations that apply to the collection, use, retention, protection, security, disclosure, transfer and other processing of personal data in the different jurisdictions in which we operate. Data collected by the camera of our solutions during the development cycle of a project may include personal information such as license plate numbers of other vehicles, facial features of pedestrians, appearance of individuals, GPS data, and geolocation data in order to train the data analytics and AI technology equipped in our solutions for the purpose of identifying different objects and predicting potential issues that may arise during the operation of a motor vehicle. Our data-collection processes implement strict methodologies to comply with data protection and privacy laws, including the EU General Data Protection Regulation (the “GDPR”), the UK General Data Protection Regulation, and the California Consumer Privacy Act of 2018 (the “CCPA”), as amended by the California Privacy Rights Act of 2020 (the “CPRA”).
We leverage systems and applications that are spread over the countries in which we do business, requiring us to regularly move data across national borders. As a result, we are subject to a variety of laws and regulations in the United States, China, the European Union, and other foreign jurisdictions as well as contractual obligations, regarding data privacy, protection, and security.
The scope and interpretation of the laws and regulations that are or may be applicable to us are often uncertain and may be conflicting, particularly with respect to foreign laws. We are subject to the GDPR, which became effective in May 2018. EU member states have enacted certain implementing legislation that adds to and/or further interprets the GDPR requirements. The GDPR, together with national legislation, regulations and guidelines of the EU member states governing the processing of personal data, impose strict obligations and restrictions on the ability to collect, use, retain, protect, disclose, transfer, and otherwise process personal data with respect to EU data subjects. In particular, the GDPR includes obligations and restrictions concerning the consent and rights of individuals to whom the personal data relates, the transfer of personal data out of the EEA, security breach notifications and the security and confidentiality of personal data. We are also subject to the UK General Data Protection Regulation (i.e., a version of the GDPR as implemented into UK law), exposing us to two parallel regimes with potentially divergent interpretations and enforcement actions for certain violations. While the European Commission issued an adequacy decision intended to last for at least four years in respect of the UK’s data protection framework, enabling data transfers from EU member states to the UK to continue without requiring organizations to put in place contractual or other measures in order to lawfully transfer personal data between the territories, the relationship between the UK and the EU in relation to certain aspects of data privacy and security law remains unclear. Other countries have enacted or are considering enacting similar cross-border data transfer rules or data localization requirements.
Additionally, on June 28, 2018, California enacted the CCPA, which came into effect on January 1, 2020. The CCPA creates individual privacy rights for California residents and increases the privacy and security obligations of entities handling personal data of California consumers and meeting certain thresholds. Further, the CPRA, which was enacted in November 2020 and became effective on January 1, 2023, significantly amends the CCPA and imposes additional data protection obligations on covered businesses, including additional consumer rights processes, limitations on data uses, new audit requirements for higher risk data, and opt outs for certain uses of sensitive data. The CPRA also created a new California data protection agency authorized to issue substantive regulations, which could result in increased privacy and information security enforcement. In addition, many similar laws have been proposed at the federal level and in other states. State laws are changing rapidly and there is discussion in Congress of a new federal data protection and privacy law to which we would become subject if it is enacted.
In China, the PRC Cyber Security Law became effective on June 1, 2017. The Cyber Security Law reaffirms the basic principles and requirements specified in other existing laws and regulations on personal information protection, such as the requirements on the collection, use, processing, storage, and disclosure of personal information. Specifically, it requires that network operators take technical measures and other necessary measures in accordance with applicable laws and regulations and the compulsory requirements of the 

29



national and industrial standards to safeguard the safe and stable operation of its networks, maintain the integrity, confidentiality, and availability of network data, take technical and other necessary measures to ensure the security of the personal information they have collected against unauthorized access, alteration, disclosure, or loss, and formulate contingency plans for network security incidents and remediation measures. It also requires a subset of network operators that meet certain thresholds to be critical information infrastructure operators (“CIIO”) to store personal information and important data collected and generated during its operation within the territory of China locally on servers in China.
Our Competition 
The ADAS and autonomous driving industries are highly competitive. In the ADAS and consumer AV market, we face competition primarily from other external providers including Tier 1 automotive suppliers and silicon providers, as well as in-house solutions developed by the OEMs to a certain extent. Our Tier 1 customers may be developing or may in the future develop competing solutions. For example, certain of our competitors have announced that they are operating autonomous robotaxis. Tier 1 automotive supplier competitors include Bosch, Continental, and Denso. Our silicon provider competitors include Ambarella, Advanced Micro Devices, Arriver / Qualcomm, Black Sesame Technologies, Horizon Robotics, Huawei, NVIDIA, NXP, Renesas Electronics, and Texas Instruments. OEMs who have or are pursuing their own in-house solutions are also indirect competitors, with Tesla and Mercedes-Benz being examples of automakers taking that approach today, with others such as General Motors, NIO, Volvo Cars, and Xpeng Motors also pursuing in-house solutions for portions of the ADAS software stack. In the future, our indirect competitors could become direct competitors. 
In the autonomous driving market, including AMaaS and consumer AV, we face competition from technology companies, internal development teams from the automakers themselves, sometimes in combination with investments in early-stage autonomous vehicle technology companies, Tier 1 automotive companies, as well as robotaxi providers. AMaaS competitors include Aurora, Cruise, Motional, Pony.ai, Waymo, Yandex, and Zoox in the United States and Europe and Auto X, Baidu, Deeproute.ai, Didi Chuxing, Momenta, and WeRide in China. Consumer AV competitors include Sony, and Tesla, who are developing self-driving vehicles for consumers. 
Developing effective ADAS technology is technologically complex, requires the development of large validation datasets in order to train the required software algorithms effectively, requires a long-term commitment to validation and qualification with an OEM before series production can even begin, and requires significant financial resources. In addition, our tightly coupled software and hardware solutions, which are based on highly advanced, road-tested, sensing and perception technologies from decades of leadership in computer vision and powered by our mission critical software and purpose-built EyeQ® family of SoCs are extremely hard to replicate. 
Moovit competes against urban mobility applications and MaaS solutions which provide transportation services and navigation data to consumers. Moovit’s free application competition includes Alphabet, Apple, Citymapper, and Transit. Moovit’s application also competes with on-demand service providers that provide multi-modal ride services and route planning through their own services including Lyft, TransLoc, Trapeze, Uber, and Via. 
The principal competitive factors impacting the market for our solutions include: 
●completeness of our technology platform including SoCs, sensing and perception technologies, sensor fusion architecture, high-precision mapping system, and supporting software and algorithms; 
●ability to design and develop ADAS and autonomous driving solutions that meet our customers’ needs; 
●automotive quality standards, compliance, and performance in all areas of ADAS and autonomous driving; 
●agile software validation and robust product release discipline; 
●scalability, and cost efficiency of our solutions; 
●engineering capabilities, the ability to innovate and continuously improve our technology; 

30



●pricing; 
●design and development support for our customers; 
●manufacturing reliability and the ability to make on-time delivery of appropriate quantities of product at a consistent level of quality; 
●ability to meet regulatory requirements; 
●intellectual property protection; and 
●brand and reputation, including the ability to market new offerings. 

We believe we compete favorably with respect to these factors. In addition, as the ADAS and autonomous driving markets progress and, in some use cases, converge, we believe we will be in a favorable position to achieve meaningful business wins given our differentiated capabilities. 
Distribution and Marketing 
Our products are sold directly to customers throughout the world, or through distribution channels for our aftermarket products meant for vehicles that do not come pre-equipped with ADAS technology. 
We actively promote our brand and technologies to increase awareness and generate demand through direct marketing as well as co-marketing programs. Our direct marketing to consumers and businesses primarily includes trade events, industry and consumer communications and press relations. We work closely with our existing customers in order to ensure that we are aware of their requirements and plans for future car models and can respond promptly and effectively. 
We regularly present our technology to regulators and safety organizations to demonstrate its capabilities and reliability and to help ensure that they develop regulations and ratings that address the full range of benefits that we believe we can offer.
Research and Development 
We believe our strong research and development is our principal competitive strength and has led to our position in the market. Our research and development activities are predominantly conducted in Israel. We have more than 80% of our full time-equivalent employees engaged in research and development, many of whom have been with the company for significant tenures. Our research and development efforts focus on algorithms, including visual processing, camera control, vehicle control, camera/radar fusion, autonomous driving sensing technologies, REM™ technology, driving policy and related engineering tasks as well as application software, silicon design and hardware electronics design. We believe we have a unique approach by developing ADAS and autonomous solutions simultaneously, giving us a technical and scale advantage over our competition. 
Our Employees
As of December 31, 2022, we had approximately 3,500 employees operating across eight countries, with approximately 80% of such employees involved in research and development and approximately 3,200 of such employees operating in Israel. None of our employees is represented by a labor union with respect to his, her or their employment. In certain countries in which we operate, we are subject to, and comply with, local labor law requirements, which may automatically make our employees subject to industry-wide collective bargaining agreements. We have not experienced any work stoppages and we consider our relations with our employees to be good.

31



Intellectual Property
Our ability to compete effectively depends in part on our ability to develop and maintain the proprietary aspects of our technology. Our policy is to obtain appropriate proprietary rights protection for any potentially significant new technology acquired or developed by us. As of December 31, 2022, we held 285 U.S. patents, 40 European patents, 209 U.S. patent applications, 486 European and other non-U.S. patent applications, and provisional patent filings. We do not view any single patent or patent application to be material.
In addition to patent laws, we rely on copyright and trade secret laws to protect our proprietary rights. We attempt to protect our trade secrets and other proprietary information through agreements with OEMs, distributors, other customers and suppliers, proprietary information agreements with our employees and consultants, and other similar measures. Our primary trademarks are for our name and product names. We cannot be certain that we will be successful in protecting our proprietary rights. While we believe our patents, patent applications, software and other proprietary know-how have value, changing technology makes our future success dependent principally upon our ability to successfully achieve continuing innovation. 
Litigation may be necessary in the future to enforce our proprietary rights, to determine the validity and scope of the proprietary rights of others, or to defend us against claims of infringement or invalidity by others. An adverse outcome in such litigation or similar proceedings could subject us to significant liabilities to third parties, require disputed rights to be licensed from others or require us to cease marketing or using certain products, any of which could have a material adverse effect on our business, financial condition, and results of operations. In addition, the cost of addressing any intellectual property litigation claim, both in legal fees and expenses, as well as from the diversion of management’s resources, regardless of whether the claim is valid, could be significant and could have a material adverse effect on our business, financial condition, and results of operations. 
Relationship with Intel
Prior to the Mobileye IPO, Intel beneficially owned 100% of our outstanding shares of common stock and we operated as Intel’s wholly owned subsidiary. As of December 31, 2022, Intel beneficially owns all of the outstanding shares of our Class B common stock, representing approximately 99.3% of the voting power of our common stock. As a result, Intel is able to control all matters submitted to our stockholders for approval, including the election of our directors and the approval of significant corporate transactions. Furthermore, in addition to any other vote required by law or by our amended and restated certificate of incorporation, until the first date on which Intel ceases to beneficially own 20% or more of our outstanding shares of common stock, the prior affirmative vote or written consent of Intel as the holder of our Class B common stock will be required in order for us to: adopt or implement any stockholder 

32



rights plan or similar takeover defense measure; consolidate or merge with or into any other entity; permit any of our subsidiaries to consolidate or merge with or into any other entity, with certain exceptions; acquire the stock or assets of another entity for consideration in excess of $250,000,000 other than transactions in which we and one or more of our wholly owned subsidiaries are the only parties; issue any stock or other equity securities except to our subsidiaries, pursuant to the Mobileye IPO, or pursuant to our employee benefit plans limited to a share reserve of 5% of the outstanding number of shares of our common stock on the immediately preceding December 31; make or commit to make any individual or series of related capital or other expenditures in excess of $250,000,000; create, incur, assume or permit to exist any indebtedness or guarantee any indebtedness in excess of $250,000,000; make any loan to or purchase any debt securities of any person in excess of $250,000,000; redeem, purchase or otherwise acquire or retire for value any equity securities of our company except repurchases from employees, officers, directors or other service providers upon termination of employment or through the exercise of any right of first refusal; take any actions to dissolve, liquidate, or wind-up our company; declare dividends on our stock; or amend, terminate or adopt any provision inconsistent with our amended and restated certificate of incorporation or amended and restated bylaws. See “Item 1A. Risk Factors — Risks Related to our Relationship with Intel and our Dual Class Structure”.
We and Intel continue to interact as strategic partners, collaborating on projects to pursue the growth of computing and advanced technology in the automotive sector. In connection with the Mobileye IPO, we entered into certain agreements (collectively, the “Intercompany Agreements”) with Intel and certain of its subsidiaries that provide the framework for our ongoing relationship with Intel, including the Master Transaction Agreement, which contains key provisions relating to our ongoing relationship with Intel. The Master Transaction Agreement also contains agreements relating to the conduct of the Mobileye IPO and future transactions, and governs the relationship between Intel and Mobileye. Unless otherwise required by the specific provisions of the Master Transaction Agreement, the Master Transaction Agreement will terminate on a date that is five years after the first date upon which Intel ceases to beneficially own at least 20% of our outstanding shares of common stock. The provisions related to our cooperation with Intel in connection with future litigation will survive seven years after the termination of the agreement, and certain other provisions including those related to indemnification by us and Intel will survive indefinitely.
Key provisions of the Master Transaction Agreement include: we will provide Intel, after the date that is 180 days after the closing of the Mobileye IPO or such earlier date as provided in the Master Transaction Agreement, with certain registration rights to register our common stock, because the shares of our common stock held by Intel after the Mobileye IPO are “restricted securities” as defined in Rule 144 under the Securities Act; we will cooperate with Intel, at its request, to accomplish a distribution by Intel of our common stock to Intel stockholders which is intended to qualify as a distribution under Section 355 of the Code, or any corresponding provision of any successor statute, and we have agreed to promptly take any and all actions reasonably necessary or desirable to effect any such distribution, in which Intel will determine, in its sole and absolute discretion, whether to proceed with all or part of the distribution, the date of the distribution and the form, structure and all other terms of any transaction to effect the distribution; so long as Intel beneficially owns at least 20% of our common stock, we will sell Intel our commercially available products, including EyeQ® SoCs, for internal use, but not for resale on a standalone or bundled basis; we and Intel agree to hold the other in most favored status with respect to products purchased or sold for internal use, meaning that the product prices, terms, warranties and benefits provided between us and Intel shall be comparable to or better than the equivalent terms being offered by the party providing the products to any single, present customer of such party; we have granted Intel a continuing right to purchase from us shares of Class A common stock or Class B common stock as is necessary for Intel to maintain an aggregate ownership interest of our common stock representing at least 80.1% of our common stock outstanding; we and Intel have cross-indemnities that generally place the financial responsibility on us and our subsidiaries for all liabilities associated with the current and historical Mobileye business and operations, and generally will place on Intel the financial responsibility for liabilities associated with all of Intel’s other current and historical businesses and operations, in each case regardless of the time those liabilities arise, and certain other indemnities; the Master Transaction Agreement contains a general release for liabilities arising from events occurring on or before the time of the Mobileye IPO; for so long as Intel provides us with accounting and financial services under the Administrative Services Agreement that we entered into with Intel, and to the extent necessary for the purpose of preparing financial statements or completing a financial statement audit, we will provide Intel as much prior notice as reasonably practical of any change in the independent certified public accountants to be used by us or our subsidiaries for providing an opinion on our consolidated financial statements; until the later of Intel ceasing to be a “controlling person” of us as defined in the Securities Act and such date that Intel ceases to provide us with legal, financial, or accounting services under the Administrative Services Agreement, we will comply with all Intel rules, policies, and directives identified by Intel as critical to legal and regulatory compliance, to the extent such rules, policies, and directives have been previously communicated to us, and will not adopt legal or regulatory policies or directives inconsistent with the policies identified by Intel as critical to legal and regulatory compliance; for a period of two years following the closing of the Mobileye IPO, we and Intel will not, directly or indirectly, solicit active employees of the other without prior consent by the other, provided we both have agreed to give such consent if either party believes, in good faith, that consent is necessary to avoid the resignation of an employee from one party that the other party would wish to employ; all outstanding options to 

33



purchase shares of Intel and all other Intel equity awards held by Mobileye Group employees at the time of the Mobileye IPO will continue to be outstanding until the earliest of (i) the date the award is exchanged pursuant to any issuer exchange offer undertaken by us and Intel, (ii) the date the award is exercised or expires under the terms of the applicable award agreement, and (iii) the date such award is canceled as a result of a Mobileye Group employee being terminated or, if later, the end of any post-termination exercise period specified in the award agreement or by the applicable equity plans’ administrative committees; immediately after completion of the Mobileye IPO (and after giving effect to the repayment of indebtedness by us to Intel and other transactions that occurred substantially concurrently with the Mobileye IPO), Intel agreed to ensure that we had $1.0 billion in cash, cash equivalents, or marketable securities; and Intel will use commercially reasonable efforts to provide three months’ advance notice to our board of directors in the event that Intel intends to pursue a transaction (even if no such transaction is imminent or probable at such time) which is reasonably expected to cause Intel’s ownership in us to fall below 50% of our total issued and outstanding shares of common stock.
In connection with the Mobileye IPO, we entered into a LiDAR Product Collaboration Agreement with Intel and a Technology and Services Agreement with Intel pursuant to which Intel granted us a limited license to sensitive core technology relating to lidar and radar, respectively. Pursuant to the LiDAR Product Collaboration Agreement, the license is limited to a particular lidar sensor system for ADAS and AV systems in automobiles and to certain types of customers (Tier 1s, OEMs and MaaS), and the development by us of any future products based on Intel technology will depend on future agreements. Further, we are not licensed to manufacture products based on Intel technology with anyone other than Intel. Pursuant to the Technology and Services Agreement, the license is limited to the development of a specific type of radar for specific applications, and any radar products that do not fall under the scope of the agreement will require a separate license from Intel, at Intel’s discretion. As a result, we will not own most new lidar and radar intellectual property, even if developed solely by us. If we are not able to continue to use or license sensitive core technology related to lidar and radar from Intel, we may not be able to secure alternatives in a timely manner or at all, and our ability to remain competitive would be harmed and that could adversely affect our business, results of operations and financial condition. See “Item 1A. Risk Factors — Risks Related to our Relationship with Intel and our Dual Class Structure — We may have conflicts of interest with Intel and, because of (i) certain provisions in our amended and restated certificate of incorporation relating to related person transactions and corporate opportunities, (ii) agreements we have with Intel in connection with the Mobileye IPO, and (iii) Intel’s controlling beneficial ownership interest in our company, we may not be able to resolve such conflicts on terms favorable to us.”
Several of our directors also serve as officers, directors and/or other positions at Intel. Mr. Gelsinger, the Chair of our Board of Directors, is the Chief Executive Officer and a director of Intel. Ms. Pambianchi, our director, is an Executive Vice President and the Chief People Officer of Intel. Mr. Huntsman, our director, is the co-chair of the Government Affairs Advisory Committee of Intel. Mr. Yeary, our director, is a director of Intel.
See the information under the heading “Item 13. Certain Relationships and Related Transactions, and Director Independence” which is incorporated herein by reference from our definitive proxy statement for the 2023 Annual Meeting of the Stockholders (the “2023 Proxy Statement”), which we expect to file with the SEC within 120 days after the end of our fiscal year ended December 31, 2022.
Available Information
Our reports filed with or furnished to the Securities and Exchange Commission (“SEC”) pursuant to Sections 13(a) and 15(d) of the Securities Exchange Act of 1934, as amended (“the Exchange Act”), are available, free of charge, on our Investor Relations website at https://ir.mobileye.com/ as soon as reasonably practicable after we electronically file such material with, or furnish it to, the SEC. The SEC maintains an Internet site (www.sec.gov) that contains all of the documents we file with the SEC. 
Information about our Executive Officers
Set forth below are the names, ages and positions as of the date hereof of our executive officers.

Name | | Age | | Position | 
Amnon Shashua | ​ | 62 | ​ | Chief Executive Officer, President, and Director | ​
Anat Heller | ​ | 45 | ​ | Chief Financial Officer | ​
Gavriel Hayon | ​ | 53 | ​ | Executive Vice President, Research and Development | ​
Shai Shalev-Shwartz | ​ | 47 | ​ | Chief Technology Officer | ​
Nimrod Nehushtan | ​ | 33 | ​ | Senior Vice President Business Development & Strategy, Co-Manager REM | ​

​

34



Amnon Shashua is our co-founder and has been serving as our Chief Executive Officer and President since 2017 and as our director since our founding in 1999. He served as a Senior Vice President at Intel from 2017 to 2022, following our acquisition by Intel. Professor Shashua founded Mobileye in 1999. In addition to Mobileye, Professor Shashua has founded a number of startups in the fields of computer vision and machine learning, including CogniTens, which develops comprehensive dimensional measurement systems, which he founded in 1995 and has since been acquired, OrCam, which harnesses computer vision and AI to assist the visually and hearing impaired, which he co-founded in 2010 and serves as its Co-Chairman, and AI21 Labs, which works to use AI to understand and create natural language, which he co-founded in 2017 and serves as its Chairman. In 2019, Professor Shashua founded One Zero Digital Bank, a digital bank in Israel. In December 2021, Professor Shashua co-founded Mentee Robotics, which aims to build humanoid robots and has since been serving as its Chairman. Professor Shashua holds the Sachs Chair in Computer Science at the Hebrew University of Jerusalem, where he teaches and supervises graduate students. He has published 162 papers in the field of machine learning and computational vision and holds over 94 patents. Professor Shashua has been awarded prestigious prizes for his contributions to science and technology and is also the 2020 Dan David laureate in the field of AI awarded for his ground-breaking work in the field. In 2019, he was recognized as the Electronic Imaging Scientist of the Year by the Society for Imaging Science and Technology. Professor Shashua and his team were also finalists in the European Inventor Awards of 2019, awarded by the European Patent Office. In July 2022, Professor Shashua received the Mobility Innovator Award from the Automotive Hall of Fame. 
Anat Heller has been serving as our Chief Financial Officer since 2018. Prior to her current position, Ms. Heller joined Mobileye in 2008 as our Corporate Controller and became our Director of Finance in 2016. Prior to joining Mobileye, Ms. Heller served as the deputy corporate controller at Lipman Electronics Engineering (formerly Nasdaq, TASE: LPMA), which was acquired by Verifone (NYSE: PAY). Ms. Heller was previously a senior associate at PricewaterhouseCoopers Israel. Ms. Heller earned her B.A. from The College of Management Academic Studies in Israel and is a licensed certified public accountant.
Gavriel Hayon has been serving as our Executive Vice President, Research and Development since 2018. Dr. Hayon joined Mobileye in 1999 as an algorithm developer and since then led teams responsible for computer vision algorithms and led the algorithms department. In 2004, Dr. Hayon became the Vice President of Research and Development, leading the development of and bringing to production multiple ADAS products. From 2017 to 2022, following our acquisition by Intel, Dr. Hayon served as a Vice President of Intel. Prior to his work at Mobileye, Dr. Hayon was an algorithms developer at Applied Materials (Nasdaq: AMAT). Dr. Hayon received his Ph.D. in AI from the Hebrew University, his M.Sc. in physics from the Weitzman Institute and his B.Sc. degree in physics from the Technion Israel Institute of Technology.
Shai Shalev-Shwartz has been serving as our Chief Technology Officer since 2018. From 2017 to 2022, following our acquisition by Intel, Professor Shalev-Shwartz served as a Senior Fellow of Intel. Professor Shalev-Shwartz is well known for his research in machine learning and was listed as one of the 100 most influential researchers worldwide in 2016 by AMiner. Professor Shalev-Shwartz is also a professor at the Rachel and Selim Benin School of Computer Science and Engineering at the Hebrew University of Jerusalem. In 2014, he co-authored a book used by major universities on theoretical machine learning: “Understanding Machine Learning From Theory to Algorithms.” Before joining Hebrew University and Mobileye, Mr. Shalev-Shwartz was a research assistant professor at Toyota Technological Institute in Chicago, and also worked in research at both Google (Nasdaq: GOOG) and IBM (NYSE: IBM). Professor Shalev-Shwartz has written more than 100 research papers, focusing on machine learning, online prediction, optimization techniques and practical algorithms. In 2020, he was awarded the prestigious Michael Bruno Award for his research and his contribution to computer science and engineering. Mr. Shalev-Shwartz earned his Ph.D. from the Hebrew University of Jerusalem.
Nimrod Nehushtan has been serving as our Senior Vice President of Business Development & Strategy and Co-Manager of REM since 2022. Prior to his current position, Mr. Nehushtan served as Co-General Manager of the REMTM division of Mobileye, overseeing product development and leading business operations and growth. Mr. Nehushtan joined Mobileye in 2017 as a Project Manager. Prior to joining Mobileye, Mr. Nehushtan was an engineer at Israel Aerospace Industries. Mr. Nehushtan earned his B.Sc. in mechanical engineering from Tel Aviv University.
​
