Prompt: Summarize the business model from the following text. Answer with a continuous text and with fivehundredtwelve tokens at max. Set your focus on sources of revenue , the intended customer base , products , distribution channels  and details of financing. Use only information from the following the text:

Item 1.  Business
Overview
 MoSys, Inc., together with its subsidiaries (“MoSys,” the “Company,” “we,” “our” or “us”), is a fabless semiconductor company focused on the development and sale of integrated circuits, or ICs, for the high-speed cloud networking, communications, security appliance, video, monitor and test, data center and computing markets. Our solutions deliver time-to-market, performance, power, area and economic benefits for system original equipment manufacturers, or OEMs. Our primary product line is marketed under the Blazar Accelerator Engine name and comprises our Bandwidth Engine and Programmable HyperSpeed Engine, or PHE, IC products, which integrate our proprietary, 1T-SRAM high-density embedded memory and a highly-efficient serial interface protocol resulting in a monolithic memory IC solution optimized for memory bandwidth and transaction access performance.   Further performance benefits can be achieved to offload statistical, search or other custom functions using our optional integrated logic and processor elements.  As data rates and the amount of high-speed processing increase, critical memory access bottlenecks occur. Our Bandwidth Engine and PHE ICs dramatically increase memory accesses per second, removing these bottlenecks. In addition, the serial interface and high-memory capacity reduce the board footprint, number of pins and complexity, while using less power. Our LineSpeed IC product line comprises non-memory, high-speed serialization-deserialization interface, or SerDes I/O, physical layer, or PHY, devices that ensure signal integrity between interfaces which is commonly referred to as clock data recovery, or CDR, or retimer functionality, which perform multiplexing to transition from one speed to another, commonly referred to as Gearbox functionality. These PHY devices reside within optical modules and networking equipment line cards designed for next-generation Ethernet and optical transport network applications.
Industry Background
 The amount of data and the number of  data consumers and devices is growing exponentially, driven primarily by commercial and consumer cloud applications, video services, high speed mobile networks, Internet of Things, or IoT, and many other cloud applications. In order to meet these demands, the new cloud infrastructure, including the backbone, edge, access network and data centers, must scale in both speed and intelligence to handle real-time security, bandwidth allocation, and service-level expectations. In addition, workloads or applications delivered at a massive scale from the cloud require flexible and efficient data transmission to optimize resources to enable these applications and lower the overall cost, size and power of the data center. These increased demands strain communication between onboard IC devices, limiting the data throughput in network switches and routers and the network backbone.
 3


To meet these demands, carrier and enterprise networks are merging with the cloud and are undergoing significant changes and, most significantly, are migrating to packet-based Ethernet networks that enable higher throughput, lower cost and uniform technology across access, core and metro network infrastructure. These networks are now being designed to deliver voice, video and high-speed Internet services on one converged, efficient and flexible network. These trends require networking systems, especially the high-speed switches, security appliances and routers that primarily comprise these networks, to comply with evolving market requirements and be capable of providing new services and better quality of service while supporting new protocols and standards. To support these trends, traditional OEM network and telecommunications equipment manufacturers, such as Nokia Corporation, and its subsidiary, Alcatel-Lucent, Cisco Systems, Inc., Tel. LM Ericsson, Fujitsu Ltd., Hitachi Ltd., Huawei Technologies, and Juniper Networks, Inc., as well as new vendors and cloud-service providers, who are delivering a new set of white-box solutions, must offer higher levels of packet forwarding rates, bandwidth density and be optimized to enable higher-density, lower-power data path connectivity in the next generations of their networking systems.
 Networking communications, security, video and computing systems throughout the cloud network must operate at higher speed and performance levels, and so require new generations of packet processors and improved memory subsystems to enable system performance. These systems and their component line cards generally need to support aggregate rates of 100 gigabits per second, or Gbps, and above to meet the continued growth in network traffic. Data centers and access equipment that were previously aggregating slower traffic at rates of up to 40 Gbps now are being designed to aggregate traffic at 100 Gbps, or more. The transition to 100 Gbps networks is underway, and the increase in data rates for these networks is expected to continue to grow rapidly over the coming years. 
 Several types of semiconductors are included on each line card, including one or more processors and multiple memory chips. These processors are complex ICs or IC chipsets that perform high-speed data or packet processing for functions, such as traffic routing, shaping, metering, billing, statistics, detection, steering, security, video processing, monitoring and workload acceleration. The line cards use various types of memory ICs to facilitate temporary packet storage and assist in the analysis and tracking of information embedded within the data flowing through the processors. After a packet enters the line card, a packet or data processor helps separate the packet into smaller pieces for rapid analysis. In a typical packet-based network for example, the data is broken up into the packet header, which contains vital information on packet destination and type, such as the Internet protocol address, and the payload, which contains the data being sent. Generally, the line card operations must occur at full data rates and typically require accessing memory ICs many times. Simultaneously, the packet’s payload, which may be substantially larger than the packet header, is also stored in memory ICs until processing is complete and the packet can re-combine and be sent to its next system destination. Within the line card, communication between the packet processor and memory ICs occurs through an interface consisting of combinations of physical pins on each type of chip. These pins are grouped together in a parallel or a serial architecture to form a pathway, called a bus, through which information is transferred from one IC to the next.
 Today, the majority of physical buses that connect networking equipment and components use a parallel architecture to communicate between processors and memory ICs, which means information can travel only in one direction and in one instance at a time. As processing speeds increase, the number of pins required and the speed of the bus in a parallel architecture become a limitation on system performance and capability. In contrast, the number of connections is reduced substantially across fewer, higher-rate pins in a serial architecture, and data is transferred simultaneously in both directions. Data transfer rates are limited by the data access rates of the various ICs included on the line card, thus leading to bottlenecks when these ICs perform inadequately. In order to remove these bottlenecks and meet next-generation bandwidth requirements, the line card ICs need to support higher access rates enabled by internal memory or high-speed serial bus architectures and these more advanced interface protocols.
 Most networking and communication systems sold and in operation today include line cards that process data at speeds ranging from 10 Gbps to 400 Gbps, and support many aggregated slower ports. To accommodate the substantial and growing increase in demand for networking communications and applications, networking systems manufacturers are developing and bringing to market next-generation systems that run at aggregate speeds of 400 Gbps or more with newer products scaling to tens of thousands of Gbps, or tens of terabits, per second. Applications, such as security appliances, broadcast video, compute accelerators etc. that were previously running at aggregate rates of 10G or 40G, are moving to higher aggregate rates in the 100s of gigabits.   Although processor performance in applications, such as computing and networking has traditionally doubled nearly every 18 months, or even sooner, 
 4


the performance of external high-density memory technology has generally been able to double only once every 10 years. Existing memory IC solutions built for high capacity and based on parallel interface architecture struggle to meet the access rates required to meet speeds of 100 Gbps and beyond due to system-level limitations for pin counts, power and performance. To compensate for slow external memory access, developers must either integrate larger amounts of on-chip memory and/or utilize complex system alternatives to try to work around the access-rate limitations of these memories. The additional memory and circuitry adds to IC power, size and cost and may not be feasible depending on the economics and technology used to implement the data processor. These networking and communications systems generally comprise a chassis populated by four to 16 line cards. Often, these systems are shipped to customers with only a portion of the line card slots populated, and the customer will add additional line cards to increase system performance, capacity and features.
 Each line card requires a significant amount of memory to support its processing capabilities. Traditional external memory IC solutions currently used on line cards include both dynamic random access memory, or DRAM, and static random access memory, or SRAM. Line cards in networking systems use both specialized, high-performance DRAM ICs, such as reduced-latency DRAM, or RLDRAM, low-latency DRAM, or LLDRAM, and commodity DRAM, such as double data rate, or DDR ICs.  The latest DDR memory is high-bandwidth memory, or HBM, provides high bandwidth, but has fundamentally slow access time.  For very high access, networking systems use higher-performance SRAM, which may be integrated into the data processing IC itself depending on size, power and economics or using traditional external SRAM IC, such as quad data rate, or QDR SRAM. These memories are very fast, but are much smaller, cost more and burn more power than traditional DRAM. Substantially all of these traditional memory IC solutions use parallel interfaces, which are slower than serial interfaces. For data processing solutions, which are unable to integrate large amounts of SRAM, such as field-programmable gate arrays (FPGA), we believe the traditional external SRAMs or RLDRAMs will be increasingly challenged to meet the performance, pin count, area and power requirements as networking systems and other new security, video, and compute systems expand beyond 100 Gbps. The result is a gap between processor and memory performance. To meet the higher performance requirements being demanded by the industry, while using current components and architectural approaches, system designers must add more discrete memory ICs to the line cards and/or add more embedded memory on the packet processor. New processor and custom data processing engine ICs are being developed that integrate more SRAM to help offset the bottlenecks, but the cost to develop these custom ICs is high and there is a trade-off in cost, power and size.  FPGAs offer flexibility, lower development cost and time to market but are limited in the amount of internal circuitry and the amount of integrated SRAM memory. We believe our Bandwidth Engine family of products is well suited to address memory access bottleneck challenges and provide significant performance, size, pin count and power advantages compared to traditional external memory solutions, especially for FPGA-based systems.
 In order to improve performance and resolve memory bottlenecks, there is an emerging trend in which computations are performed by algorithms on the memory device in order to reduce processing time and power consumption. This trend is sometimes called in-memory compute or processor-in-memory. In order to make a flexible solution, the in-memory compute can be accomplished with arrays of reduced instruction set computer (RISC) cores on the memory device. Further performance gains can be accomplished with application-specific enhancements to the memory device’s instruction set architecture. 
 We have developed our ICs to synergistically address the need for high-speed data access and throughput currently confronting system designers. We expect our IC products to meet the increasing demands placed on conventional memory technology used on the line cards in high-speed systems. We believe that our products and technology are well positioned as replacements for existing IC solutions in order to support the needs of a growing number of FPGA-based data processing applications with aggregate rates greater than 100 Gbps that require high bandwidth and high access rate to memory.
 5


Our Approach
 We have leveraged our proprietary intellectual property, or IP, to design our IC products to help networking OEMs address the growing bottlenecks in system performance. We have incorporated critical features into our product families to accomplish this objective.
 On-Chip Acceleration
 One significant performance bottleneck in any network line card is the need to transfer data between discrete ICs. Many of these data-transfer operations are iterative in nature, requiring subsequent, back-to-back accesses of the memory IC by the processor IC. Our Bandwidth Engine ICs include an arithmetic logic unit, or ALU, which enables the performance of mathematical operations on data. Moving certain processing functions from the processor IC to the Bandwidth Engine IC through the use of this embedded ALU, reduces the number of processing transactions and frees the processor IC to perform other important networking or micro-processing functions.
 The PHE, which we formerly called our programmable search engine, or PSE IC, takes this concept one step further by incorporating integrated RISC processors optimized for processing data structures and graphs.  The processors can be programmed by the user to offload and accelerate standard and/or customized functions from the main processor thereby reducing memory transactions and data path complexity to provide improved performance and lower system latency.
 High-Performance Interface
 High-speed, efficient interfaces are critical building blocks to meet high data transfer rate requirements for communication between ICs on network line cards. Semiconductor companies are increasingly turning to serial interface architectures to achieve needed system performance. For example, high-performance ICs that are sold into wide markets, such as field programmable gate arrays, or FPGAs, and network processing units, NPUs, are using serial interfaces to ensure they can compete with custom designed application specific ICs, or ASICs, by matching their performance. Using serial interfaces, IC developers also are able to reduce pin count (the wired electrical pins that connect an IC to the network line card on which it is mounted) on the IC. With reducing geometries, the size of most high-performance ICs is dictated by the number of pins required, rather than the amount of logic and memory embedded in the chip. As a result, using serial interface facilitates cost reduction and reduced system power consumption, while improving the performance of both the IC itself and the overall system. While serial interfaces provide significantly enhanced performance over parallel interfaces, SerDes interfaces traditionally have had higher power consumption, which is a challenge for IC designers. Our SerDes interfaces, however, are optimized to meet our customers’ signal integrity, low-power consumption and latency requirements.
 We make our interface technologies compliant with industry standards so that they can interoperate with interfaces on existing ICs. In addition, we make them programmable to support multiple data rates, which allows for greater flexibility for the system designer, while lowering development and validation costs. Interoperability reduces development time, thereby reducing the overall time to market of our customers’ systems.
 GigaChip Interface Protocol
 In addition to the physical characteristics of the serial interface, the protocol used to transmit data is also an important element that impacts speed and performance. To address this and complement our Bandwidth Engine and PHE devices, we have developed the GigaChip Interface, or GCI, which is an open-interface transport protocol optimized for efficient chip-to-chip communications. The GCI electrical interface is compatible with the current industry standards, including 10G and 25G IEEE and OIF interface standards, to simplify electrical interoperability between devices. GCI can enable highly efficient serial chip-to-chip communications, and its transport efficiency averages 90% for the data transfers it handles. GCI is included in our ICs and is offered to customers and prospective partners on terms intended to encourage widespread adoption.
 6


High-Performance and High-Density Memory Architecture
 The high density of our proprietary 1T-SRAM technologies stems from the use of a single-transistor, or 1T, which is similar to DRAM, with a storage cell for each bit of information. Embedded memory utilizing our 1T-SRAM technologies is typically two to three times denser than the six-transistor storage cells used by traditional SRAM, or 6T-SRAM. Embedded memory utilizing our 1T-SRAM technologies typically provides speeds essentially equal to or greater than the speeds of traditional SRAM and DRAM, particularly for larger memory sizes. Our 1T-SRAM memory designs can sustain random access cycle times of less than three nanoseconds, significantly faster than DRAM technology. Embedded memory utilizing our 1T-SRAM technologies can consume as little as one-half the active power and generate less heat than traditional SRAM when operating at the same speed. The 1T-SRAM allows us to integrate more high-performance memory using less expensive processing technology, reduces system level heat dissipation and enables reliable operation using lower-cost packaging. 
 Our PHE integrates RISC cores optimized for operating data stored in the PHE’s memory block.  The integration of the cores with memory allows system algorithms or functions to be offloaded to the device and reduces overall system-task latency and increases throughput.  New algorithms or functions can be added or adjusted to the PHE device in software. 
 Embedded In-Memory Functions (IMF)
 We have combined our high-speed memory architecture with intelligence to define an embedded memory that can execute embedded functions and algorithms internally, or “in-memory,” to allow software and hardware designers acceleration options to improve the performance of their applications.
 The IMFs executed within the memory architecture in our Bandwidth Engine and PHE products result in application-performance increases by reducing the number of external memory and computational operations need to accomplish the same functions using traditional memories.  Also, by executing in-memory, the resources of the packet processor and other ICs on the customer’s board are available to perform other functions.
 Our Strategy
 Our primary business objective is to be a profitable IP-rich fabless semiconductor company offering ICs that deliver unparalleled memory bandwidth and access rate performance for high-performance data processing in cloud networking, security appliances, video, test and monitoring, and data center systems. The key components of our strategic plan include the following strategies:
Target Large and Growing Markets
 Our initial strategy is to target the multi-billion dollar networking, telecommunications, security appliance and data center OEM equipment markets, and we have developed products to support the growth in 100 Gbps and higher networking speeds. We are currently supporting numerous customers, with whom we have achieved design wins. We continue to actively pursue additional design wins for the use of our ICs in our target markets. We believe our design wins represent the potential for future revenue growth. With limited history to date, however, we cannot estimate how much revenue each design win is likely to generate, or how much revenue all of these (and future design wins) are likely to generate. There is no assurance that these customer designs will be shipped in large volume by our customers to their customers, however.
Expand Adoption of the GigaChip Interface Protocol
 We have provided our GCI interface protocol as an open industry standard that may be designed into other ICs in the system, as we believe this will further enable serial communication on line cards and encourage adoption of our Bandwidth Engine IC products. A number of IC providers and partners have publicly announced their support of GCI and Bandwidth Engine, including the largest FPGA providers -Altera Corporation (a subsidiary of Intel Corporation) and Xilinx, Inc., and EZchip Semiconductor Ltd. (a subsidiary of Mellanox Technologies Ltd.), with whom we work closely to support common customers. In addition, multiple networking systems companies, including actual and prospective customers, have adopted GCI.
 7


Build Long-Term Relationships with FPGA Vendors and Suppliers of Data Processing Solutions
 We believe that having long-term relationships with FPGA providers is critical to our success, as such relationships enable us to reduce our time-to-market, provide us with a competitive advantage and expand our target markets. A key consideration of network system designers is to demonstrate interoperability between our IC products and the processor ICs  utilized in their systems. To obtain design wins, we must demonstrate this interoperability, and also show that our IC works optimally with the packet processor to achieve the performance requirements. In addition, our current strategy requires packet processor suppliers to adopt our GCI interface. To that end, we have been working closely with FPGA and application specific standard product providers, to enable interoperability between our Bandwidth Engine IC products and their high-performance products. To facilitate the acceptance of our Bandwidth Engine ICs, we have made available development and characterization kits for system designers to evaluate and develop code for next-generation networking systems. Our characterization kits are fully-functional hardware platforms that allow FPGA and ASIC providers, and their customers, to demonstrate interoperability of the Bandwidth Engine IC with the ASIC or FPGA the designers use within their systems. 
 Our IC Products 
 BLAZAR Accelerator Engines
 Our Blazar Accelerator Engines, include the Bandwidth Engine, which is targeted for high-performance applications where throughput is critical, and the PHE, which combines the features of the Bandwidth Engine with 32 RISC processors to allow user-defined functions or algorithms to be embedded in the PHE.
Bandwidth Engine
 The Bandwidth Engine is a memory-dominated IC that has been designed to be a high-performance companion IC to packet processors. While the Bandwidth Engine primarily functions as a memory device with a high-performance and high-efficiency interface, it also can accelerate certain processing operations by serving as a co-processor element. Our Bandwidth Engine ICs combine: (1) our proprietary high-density, high-speed, low latency embedded memory, (2) our high-speed serial interface technology, or SerDes, (3) an open-standard interface protocol and (4) intelligent access technology. We believe an IC combining our 1T-SRAM memory and serial interface with logic and other intelligence functions provides a system-level solution and significantly improves overall system performance at lower cost, size and power consumption. Our Bandwidth Engine ICs can provide up to and over 6.5 billion memory accesses per second externally and 12 billion memory accesses per second internally, which is more than three times the performance of current memory-based solutions. They also can enable system designers to significantly narrow the gap between processor and memory IC performance. Customers that design Bandwidth Engine ICs onto the line cards in their networking systems will re-architect their systems at the line-card level and use our product to replace traditional memory solutions. When compared with existing commercially available solutions, our Bandwidth Engine ICs may:

• | provide up to four times the performance;
--+------------------------------------------


• | reduce power consumption by approximately 50%;
--+-----------------------------------------------


• | reduce cost by greater than 50%; and
--+-------------------------------------


• | result in a dramatic reduction in IC pin counts on the line card.
--+------------------------------------------------------------------

 Our first-generation Bandwidth Engine IC products contain 576 megabytes, or MB, of memory and use a serial interface with up to 16 lanes operating at up to 10.3 Gbps per lane. We announced the end-of-life of these products and expect to complete fulfillment of last-time customer orders in the first half of 2019.
 Our second-generation Bandwidth Engine IC products contain 576 MB of memory and use a SerDes interface with up to 16 lanes operating at up to 12.5 Gbps per lane. In addition to a speed improvement of up to 50% over our first-generation products, the architecture has enabled multiple family-member parts with added specialized features. We have been shipping our Bandwidth Engine 2 IC products since 2013. We continue to win new designs for this device family, and expect these products to be our primary revenue source for the foreseeable future.
 8


Our third-generation Bandwidth Engine IC products contain 1152 MB of memory and use a SerDes interface with up to 16 lanes operating at up to 25 Gbps per lane. Our Bandwidth Engine 3 ICs target support for packet-processing applications with up to five billion memory single word accesses per second, as well as burst mode to enable full duplex buffering up to 400 Gbps for ingress, egress and oversubscription applications. The devices provide benefits of size, power, pin count and cost savings to our customers. We do not anticipate customer production revenues from these products until the second half of 2019 or later.
 Programmable HyperSpeed Engine (PHE)
 Our PHE IC products further leverage our proven serial interface technology and high-density integrated memory with the processor engine architecture to enable high-speed customizable search, security, and data analysis functions for networking, security, and data center applications, as well as new markets such as video and compute acceleration. Our PHE architecture features 32 search-optimized processor engines, data flow schedulers, and over a terabit of internal access bandwidth. The device leverages our GCI interface technology and high-density integrated memory (1152 Mb of 1T-SRAM embedded memory). 
 
LineSpeed Flex PHYs
 Our LineSpeed Flex family of 100G PHYs, is designed to support industry standards and includes gearbox, Multi-Link Gearbox, or MLG, and high density CDR/retimer devices designed to enable Ethernet and OTN line card applications to support the latest electrical and optical interfaces. To date, we have announced four unique devices in this product family:
  
• | MSH320, a 100Gbps Gearbox with RS-FEC: For adapting 10x10 to 4x25 from 100Gbps optical standards to a host ASIC, MAC/Framer, NPU or FPGA with 10x10G interfaces. The MSH320 includes an integrated Reed-Solomon forward error correction, or RS-FEC, option to enable systems to also support 100G electrical and optical standards. The device also includes a 10x10Gbps retimer to allow seamless support of 10 and 40Gbps interfaces;
--+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


• | MSH225, a 10 Lane Full-Duplex Retimer: For high-density retiming applications where the line rates may be up to 28Gbps per lane and connect to host ASIC, framer, NPU or FPGA ICs equipped with 25Gbps interfaces. Each one of the 20 total independent lanes can be configured to support 10, 25, 40 or 100Gbps standards. The MSH225 integrates optional 100Gbps RS-FEC capability;
--+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


• | MSH322, a 100Gbps Multi-Link Gearbox for Line Cards for support of high-density, independent 10GE and 40GE interfaces multiplexed into a 100GE (4x25Gbps) host interface, while supporting electrical and optical industry standards. The device enables line cards with high-density switches based on 25Gbps interfaces to support two times the density of 10 and 40Gbps ports; and
--+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


• | MSH321, a derivative Multi-Link Gearbox built into a highly compact package and optimized layout to support the MLG function in module and compact daughter card applications.
--+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

 
We are shipping production quantities of our LineSpeed products to a lead customer, and expect to begin generating meaningful recurring revenue from sales to this customer in 2019.
 IP Licensing and Distribution
 Historically, we have offered our memory and interface technologies on a worldwide basis to semiconductor companies, electronic product manufacturers, foundries, intellectual property companies and design companies through product development, technology licensing and joint marketing relationships. We licensed our IP technology to semiconductor companies who incorporated our technology into ICs that they sold to their customers. As a result of the change in our corporate strategy, since early 2012, our IP licensing activities have been limited, and we expect this to continue. Royalty and other revenue generated from our existing IP agreements represented 9%, 11% and 24% of our total revenue in 2018, 2017 and 2016, respectively.  Royalty revenues have been declining since 2010, and we expect them to continue to decline in 2019.
 9


Research and Development
 Our ability to compete in the future depends on successfully improving our technology to meet the market’s increasing demand for higher performance and lower cost solutions. Development of our IC products requires specialized chip design and product engineers, as well as significant fabrication and testing costs, including mask costs, as we bring these products to market.  During 2017, we substantially reduced our headcount, and currently have limited internal resources available for new IC product development, which will result in fewer product improvements and new developments. In the near term, our planned product roadmap will include software-based capabilities and features that leverage our existing base of IC products.
Sales and Marketing
 We believe that systems OEMs typically prefer to extend the use of traditional memory solutions and their parallel interfaces, despite performance and costs challenges, and are reluctant to change their technology platforms and adopt new designs and technologies, such as serial interfaces, which are an integral part of our product solutions. Therefore, our principal selling and marketing activities to date have been focused on persuading these OEMs and key component specialists that our solutions provide critical performance advantages, as well as on securing design wins with them.
 In addition to our direct sales personnel, we sell through sales representatives and distributors in the United States and Asia. We also have applications engineers who support our customer engagements and engage with the customers’ system architects and designers to propose and implement our IC and IP solutions, such as the GCI interface, to address their systems challenges.
 In the markets we serve, the time from a design win to production volume shipments can range from 18 to 36 months. Networking, communications and security appliance systems can have a product life from a few years to over 10 years once a product like ours has been designed into the system.
 Our revenue has been highly concentrated, with a few customers accounting for a significant percentage of our total revenue. For the year ended December 31, 2018, Flextronics, which primarily purchases on behalf of Palo Alto Networks, Inc. and Nokia, formerly Alcatel-Lucent, Clavis Company, formerly Kogent, our Japanese IC distributor, Palo Alto Networks and Nokia, represented 32%, 18%, 18% and 15% of total revenue, respectively. For the year ended December 31, 2017, Flextronics, Clavis Company, and Nokia, represented 46%, 17% and 11% of total revenue, respectively. For the year ended December 31, 2016, Alcatel-Lucent, Clavis Company and Taiwan Semiconductor Manufacturing Co., Ltd., or TSMC, represented 47%, 21% and 13% of our total revenue, respectively.
 Intellectual Property
 We regard our patents, copyrights, trademarks, trade secrets and similar intellectual property as critical to our success, and rely on a combination of patent, trademark, copyright, and trade secret laws to protect our proprietary rights.
 As of December 31, 2018, we held 67 U.S. and 42 foreign patents on various aspects of our technology, with expiration dates ranging from 2019 to 2036. We also held 8 pending patent applications in the U.S. and abroad. There can be no assurance that others will not independently develop or patent similar or competing technology or design around any patents that may be issued to us, or that we will be able to successfully enforce our patents against infringement by others.
 The semiconductor industry is characterized by frequent litigation regarding patent and other intellectual property rights. Our licensees or we might, from time to time, receive notice of claims that we have infringed patents or other intellectual property rights owned by others. Our successful protection of our patents and other intellectual property rights and our ability to make, use, import, offer to sell, and sell products free from the intellectual property rights of others are subject to a number of factors, particularly those described in Part I, Item 1A, “Risk Factors.”
 10


Competition
 The markets for our products are highly competitive. We believe that the principal competitive factors are:
  
• | processing speed and performance;
--+----------------------------------


• | density and cost;
--+------------------


• | power consumption;
--+-------------------


• | reliability;
--+-------------


• | interface requirements;
--+------------------------


• | ease with which technology can be customized for and incorporated into customers’ products; and
--+------------------------------------------------------------------------------------------------


• | level of technical support provided.
--+-------------------------------------

 We believe that our products compete favorably with respect to each of these criteria. Our proprietary 1T-SRAM embedded memory and high-speed serial interface IP can provide our Bandwidth Engine ICs with a competitive advantage over alternative devices. Alternative solutions are either DRAM or SRAM-based and can support either the memory size or speed requirements of high-performance networking systems, but generally not both. DRAM solutions provide a significant amount of memory at competitive cost, but DRAM solutions do not have the required fast access and cycle times to enable high-performance. The DRAM solutions currently used in networking systems include RLDRAM from Micron Technology, Inc., or Micron, LLDRAM from Renesas, DDR from Samsung Electronics Co., Ltd., Micron and others, and HBM, which is stacked memory from Samsung Electronics Co. and SK Hynix. SRAM solutions can meet high-speed performance requirements, but often lack adequate memory size. The SRAM solutions currently used in networking systems primarily include QDR or similar SRAM products from Cypress Semiconductor Corporation and GSI Technology, Inc. Most of the currently available SRAM and DRAM solutions use a parallel, rather than a serial interface. To offset these drawbacks, system designers generally must use more discrete memory ICs, resulting in higher power consumption and greater utilization of space on the line card.
 Our competitors include established semiconductor companies with significantly longer operating histories, greater name recognition and reputation, large customer bases, dedicated manufacturing facilities and greater financial, technical, sales and marketing resources. This may allow them to respond more quickly than us to new or emerging technologies or changes in customer requirements. Generally, customers prefer suppliers with greater financial resources than we have currently. Many of our competitors also have significant influence in the semiconductor industry. They may be able to introduce new technologies or devote greater resources to the development, marketing and sales of their products than we can. Furthermore, in the event of a manufacturing capacity shortage, these competitors may be able to manufacture products when we are unable to do so.
 Our Bandwidth Engine and PHE ICs 
compete with embedded memory solutions, stand-alone memory ICs, including both DRAM and SRAM ICs, ASICs designed by customers in-house to meet their system requirements, and NPUs that use significant internal memory and customer-designed software to implement tasks. Our prospective customers may be unwilling to adopt and design-in our ICs due to the uncertainties and risks surrounding designing a new IC into their systems and relying on a supplier that has limited history of manufacturing such ICs and limited financial resources. In addition, Bandwidth Engine and PHE ICs require the customer and its other IC suppliers to implement our chip-to-chip communication protocol, the GCI interface. These parties may be unwilling to do this if they believe it could adversely impact their own future product developments or competitive advantages, or, if they believe it might complicate their development process or increase the cost of their products. To remain competitive, we believe we must provide unparalleled memory IC solutions with the highest bandwidth capability for our target markets, which solutions are engineered and built for high-reliability carrier and enterprise applications. 
 Our LineSpeed PHY ICs compete with solutions offered by Broadcom Ltd., Inphi Corporation, M/A-COM Technology Solutions Holdings, Inc. and Semtech Corp., as well as other smaller analog signal processing companies. We also may compete with ASICs designed by customers in-house to meet their system requirements, as well as by optical module OEMs. The market for our LineSpeed products is highly competitive, and customers have a number of suppliers they can choose from. We must provide differentiated features with a reasonable IC power budget, while offering competitive pricing.
 11


Manufacturing
 We depend on third-party vendors to manufacture, package, assemble and test our IC products, as we do not own or operate a semiconductor fabrication, packaging or production testing facility for boards and system assembly. By outsourcing manufacturing, we can avoid the high cost associated with owning and operating our own facilities, allowing us to focus our efforts on the design and marketing of our products.
 We perform an ongoing review of product manufacturing and testing processes. Our IC products are subjected to extensive testing to assess whether their performance meets design specifications. Our test vendors provide us with immediate test data and the ability to generate characterization reports that are made available to our customers. We have achieved ISO 9001:2015 certification, and all of our significant manufacturing vendors have also achieved ISO 9001 certification.
 Employees
 As of December 31, 2018, we had 21 employees all of whom are located in the United States, consisting of 13 in research and development and manufacturing operations and 8 in sales, general and administrative functions. 
Available Information
 We were founded in 1991 and reincorporated in Delaware in September 2000. Our website address is www.mosys.com. The information in our website is not incorporated by reference into this report. Through a link on the Investor section of our website, we make available our annual reports on Form 10-K, quarterly reports on Form 10-Q, current reports on Form 8-K, and any amendments to those reports filed or furnished pursuant to Section 13(a) or 15(d) of the Securities Exchange Act of 1934, as soon as reasonably practicable after they are filed with, or furnished to, the Securities and Exchange Commission, or SEC. You can also read any materials submitted electronically by us to
 the SEC on its website (www.sec.gov), which contains reports, proxy and information statements, and other information regarding issuers that file electronically with the SEC, including us.
